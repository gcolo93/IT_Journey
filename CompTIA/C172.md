#### **TCP/IP and OSI**
The TCP/IP and OSI models describe a set of procedures that sends data from one host to another. This can be done over the internet, a network, or any other form of communication.

There are two primary models used in networking— the Transmission Control Protocol and Internet Protocol (also called the TCP/IP) model and the widely used Open Systems Interconnection or the OSI model. Let's compare these two models to understand how they work and why you might choose one over the other.

The TCP/IP model has four layers, whereas the OSI model has seven. In the TCP/IP model, the bottom layer is the Network Interface Layer and contains the physical cabling or the wireless spectrum in case of a wireless network. In the OSI model, the bottom layer consists of two segments—the physical and data link. The physical layer contains cabling and connectors. It establishes how a signal crosses a wire and / or the airwaves. The data link layer includes your network card, which has your Media Access Control or MAC address.

Next is the Network Layer in both the models where the Internet Protocol, also called IP and the Internet Control Message Protocol, known as ICMP, run. Here, you can define one protocol versus another for connecting networks.

The Transport layer in each model ensures data is delivered to the correct application. The data transfer is done either through the Transmission Control Protocol, also called TCP, or the User Datagram Protocol, also known as UDP. TCP is quite reliable in terms of delivery as it verifies the data and resends it if not delivered. UDP, on the other hand, is unreliable to the extent that it doesn't guarantee redelivery of the data. It is therefore preferred in audio streaming, where retransmitting of any lost audio after a few seconds would anyway ruin the overall experience.

Fourth in the TCP model is the Application Layer. This layer deals with all the pieces of information that are added to your application, such as the Hypertext Transfer Protocol or HTTP for a web browser, Simple Mail Transfer Protocol or SMTP for email, and the Domain Name System or DNS. In the OSI model, the Application Layer has three separate components. First, the Session Layer establishes the session and ensures things are working correctly end-to-end. Next, the Presentation Layer takes care of data encryption. And finally, the Application Layer takes care of the HTTP, DNS, and FTP.

To understand how these layers work, let's use the example of sending an email. At the sender's end, the email travels down through the model starting from the Presentation layer. And as it moves, each layer appends its header information to the email. After crossing a wired or wireless network when the email reaches the other end, the layers strip off their information, one by one, until only the email remains.

You can also refer to the layers of the OSI model by their numbers. The Physical Layer is layer 1, Data Link is 2, Network is 3, Transport 4, Session 5, Presentation 6, and Application is Layer 7. While working on networking, you might come across a router being referred to as a Layer 3 device because it operates at the Network Layer. Similarly, a network card is known as a Layer 2 device and the next generation firewall, a Layer 7 firewall.

So, this is how TCP/IP differs from the OSI model. The TCP/IP model has the Network Interface, Network, Transport, and Application layers. The OSI model, on the other hand, has the Physical, Data Link, Network, Transport, Session, Presentation, and Application layers.

TCP/IP stands for transmission control protocol/internet protocol and is a set of protocols used to connect network devices to each other.

The TCP/IP networking model is made up of four layers:

1. Application Layer: This layer is responsible for the communication protocols between nodes. The protocols in this layer include hypertext transfer protocol (HTTP and HTTPS), Secure Shell (SSH), and network time protocol (NTP), among many others.
1. Transport Layer: This layer is responsible for the end-to-end transport of data. The protocols that live in this layer are transmission control protocol (TCP) and user datagram protocol (UDP).
1. Network Layer: This layer defines the logical transmission protocols for the whole network. The main protocols that live in this layer are internet protocol (IP), internet control message protocol (ICMP), and address resolution protocol (ARP).
1. Network Interface Layer: This layer establishes how data should be physically sent through the network.

The TCP/IP model is used for practical application when locating specific protocol. It covers the same topics that will be found in the OSI model but in fewer layers.

The OSI (Open System Interconnection) model consists of a seven-layer architecture that organizes the sending of data from hosts across a network. The OSI model was developed by the International Standards Organization (ISO) after the TCP/IP model to provide greater granularity of networking assignments within the model.

The layers are:

1. Physical Layer: This layer is responsible for the physical connections of the devices in the network. This layer is implemented through the use of devices such as hubs, repeaters, modem devices, and physical cabling.
1. Data Link Layer: This layer is responsible for the error-free delivery of data to the receiving device or node. This layer is implemented through the use of devices such as switches and bridge devices, as well as anything with a network interface, like wireless or wired network cards.
1. Network Layer: This layer is responsible for the transmission of data between hosts in different networks as well as routing of data packets. This layer is implemented through the use of devices such as routers and some switches.
1. Transport Layer: This layer provides services to the application layer and receives services from the network layer. It is responsible for the reliable delivery of data. It segments and reassembles data in the correct order for it to be sent to the receiving device. It may also handle the reliable delivery of data and any retries of data that are lost or corrupted (for example, TCP does this). This layer is often called the heart of OSI.
1. Session Layer: This layer is responsible for connection establishment, session maintenance, and authentication.
1. Presentation Layer: This layer is responsible for translating data from the application layer into the format required to transmit the data over the network as well as encrypting the data for security if encryption is used.
1. Application Layer: This layer is responsible for network applications (like HTTP or FTP) and their production of data to be transferred over the network.

The OSI model is widely used throughout networking documentation and discussions. Layers are often referred to by number, not name, so memorizing the numbers and having a good understanding of each layer’s uses are essential for success in the IT community.
### **Network Devices**
A physical network may be composed of a number of different devices in order to function as needed.

In this video, we’ll talk about some devices operating at the different layers of the Open Systems Interconnection, also known as the OSI model. A switch is a data link layer device. It is available either as a standalone or integrated with other devices, such as wireless routers or modems that can plug in physical devices. A switch keeps the traffic between the source and destination ports from interfering with other devices in the network. A router operates at the network layer and can connect your home network to the Internet. Or, in a business or institutional scenario, a router can connect your web server to your database server. Firewalls can operate at multiple layers of the OSI model, including data link, network, transport, session, and application - and in certain circumstances, presentation as well. At the data link and network levels the firewall defines what traffic should go between two networks. For example, your home network and the Internet are two different networks with two separate ports, and therefore two different data link layers. You can use the firewall to filter traffic between these networks based on individual IP addresses or a range of them. At the transport layer, the firewall filter settings are port-based. For example, you can allow or block traffic from a port like the Transmission Control Protocol, also called TCP or the User Datagram Protocol, often called UDP. You can also filter traffic using a range of ports or a combination of IP addresses and ports.

The firewall that operates at the network and transport layers is called the Layer 4 firewall, and the one that works at the session, presentation, and application layers is the Layer 7 firewall. The Layer 7 firewall is also smart enough to decrypt and analyze the content flowing through, thereby adding one more level of filtering to the IP address and port combination. A firewall is thus a powerful tool that can help prevent malicious attacks like Structured Query Language, also known as SQL injection, or other malware. Firewalls can be software devices that run inside a virtual machine or physical server or can also be hardware devices like Fortinet's Fortigate device. Both the software and hardware firewalls work towards the same result. The cost of a firewall varies depending on its features, capabilities, and speed. In this video, we discussed the different devices like a switch, router, and firewall that operate at the different layers of the OSI model.

You can use a wide variety of cable types in networking.

Probably the least expensive and most common type is the Unshielded Twisted Pair or UTP. Unshielded means that these cables have no extra shielding on the outside of each wire to protect against electrical interference. A variant of UTP is the Shielded Twisted Pair, or STP, which has an extra shielding to prevent the leakage of electrical noise from one wire to another. And why are these cables called "twisted pairs?" When you strip open the insulation of these cables, you'll see pairs of wires that are twisted. The tighter they are twisted, the lesser the electrical interference between them. Also, the tighter they are twisted, the higher the category of the cable.

Twisted pair cables come in different categories. The oldest type is CAT3, which was used in old analog phones. CAT3 cables have a maximum speed of only 10 megabits per second and are unlikely to be used anywhere today. CAT4 cables are also unlikely to be seen today. They were used with the token ring standard and had a maximum speed of only 16 megabits. CAT5 cables, used in Ethernet environments typical in the '90s and early 2000s, are suitable for up to 100 megabits over a maximum distance of 100 meters. However, CAT5 cables have now been superseded by the more popular CAT5e version that can support up to gigabit speeds. Most new cables today are CAT6 and can go up to one gigabit per second for 100 meters, or 10 gigabits for 50 meters. Within a single rack of servers, you can have several CAT6 cables and connect everything over inexpensive cables with UTP because the distance is only a few meters.

Another type of cable is fiber optic. As the name suggests, these cables use light instead of electricity, which is great for running much longer distances at significantly higher speeds. Fiber optic is also more durable and secure than electrical, making them the best choice for long distances such as the transatlantic submarine cables between North America and Europe. Fiber optic cables are usually of two types: single or multi-mode. Single-mode cables have several thin glass strands (in rare cases, plastic) covered by protective insulation. As single-mode cables have thin 10-micron strands, they support a narrow range of wavelengths and higher bandwidths - making them ideal for submarine cabling across continents. Multi-mode cables are much cheaper than single mode across shorter distances. The fibers in these cables are thicker, from 50 to 100 microns. Because of their thickness, they can run a wider frequency of light over shorter distances - from a couple of thousand meters to a couple of miles.

Coaxial, often called coax, is a third type of cabling that comes in many varieties. RG-59 cables connected rooftop aerial antennas to television sets - they did not carry much bandwidth, weren't very long, and were relatively inexpensive. Today, RG-6 cables have mostly replaced the RG-59 version and connect your satellite dish to your television and support much higher bandwidth. RG-11 cables connect buildings across much longer distances. They have a thick wire in the middle, which is the conductor, surrounded by insulation, shielding, and external cabling. While RG-11 cables can go long distances because of the thick wire, they don't bend very well. There are many other cable standards, including RG-62, that are not widely used. Apart from using a coax cable from a cable modem to a wireless router or television, you are not likely to run into much coax.
### **UTP Cables**
*Figure. Example of a UTP cable.*

Unshielded twisted pair cables (UTP cables) are created when pairs of wires are twisted around each other to protect and cancel out interference from each other and outside sources. UTP cables were invented by Alexander Graham Bell in 1881 for phones and are still used today. UTP cables are widely used as analog phone cables and in copper Ethernet cables.

UTP cables come in six different standard types as defined by TIA/EIA 568. You can identify the type of cable you have by looking at the writing on the cable itself.

1. Cat3 supports up to 10 Mbps (Megabits per second) for up to 100 meters and is commonly used for phone lines today.
1. Cat4 supports 16 Mbps for up to 100 meters and is not commonly used today.
1. Cat5 is used in Ethernet LANs containing two twisted pairs allowing for up to 100 Mbps up to 100 meters between the device and the switch, hub, or router. This has been practically replaced by the Cat5e specification.
1. Cat5e doubles the number of twisted pairs to four for up to 1 Gbps (Gigabits per second) over up to 100 meters.
1. Cat6 is also used in Ethernet LANs and data centers. Cat6 is made up of four tightly woven twisted pairs (more twists per linear foot) and supports 1 Gbps for up to 100 meters or 10 Gbps for up to 55 meters.
1. Cat6a is an improvement of the Cat6 standard, supporting the same standards and lengths (with the ability to run 10 Gbps over 100 meters maximum), but using a higher quality cable that is more resistant to interference. This is most commonly used in wired networks today. 

There are several different connectors that can be connected to the end of these UTP cables; the two most common are:

- RJ11: A connector that supports two pairs of wires (four total); typically used in telephones.
- RJ45: This is an end connector typically used with Ethernet cables and supports four pairs (eight wires).
### **Coaxial Cables**
` `Figure. Example of a coaxial cable

Coaxial cables are analog cables made of copper but specifically engineered with a metal shield intended to block signal interference. This cable was patented in 1880 by Oliver Heaviside and was used as an improvement over the bare copper cables widely used in that day. The protection on the coaxial cable allows them to be laid next to metal gutters or other objects without receiving interference. Today, coaxial cables are mostly used by cable TV companies to connect their customers to the company’s facilities.

There are several types of these coaxial cables, but since they are not widely used in networking today, they are not discussed further.
### **Fiber Cables**
` `Figure. Example of a fiber-optic cable

Fiber cables, or fiber-optic cables, use glass or plastic threads within cables to transfer the data using light (lasers or LEDs) as opposed to traditional metal cables using electricity. Fiber cables are useful for high bandwidth needs, meaning they can carry more data at one time. Additionally, they transfer data digitally instead of needing to convert data between binary and analog and back using metal cables. Since computer data output is digital, this transfers data in the computer’s natural way. Fiber cables allow virtually no interference to corrupt the data and are more reliable.

Fiber cables are lighter and thinner to install but are much more expensive. However, the technology community is continually trading out old wiring for fiber cables for the advantages listed above.

There are two types of fiber cables: single-mode and multimode.

1. Single-mode cables are made up of one single glass or plastic fiber. The benefit of a single fiber cable is the ability to carry higher bandwidth for 50 times the distance of a multimode cable. This requires higher cost electronics to create the light and thus is typically used for longer distances (hundreds or thousands of kilometers) and higher bandwidth applications.
1. Multimode cables are wider in diameter due to light modes being sent across the cable. Multimode fibers are highly effective over medium distances (500 meters or less at higher speeds) and are generally used within a LAN. They are also less expensive than single-mode fiber due to the potential for use with LEDs and other lower-cost options for creating the light.

The primary connector types used today are as follows:

- **ST**: This stands for a straight tip connector. This was the most commonly used connector with multimode fiber until the mid-2000s. It was used on campuses, corporate networks, and for military purposes. Today, LC connectors are usually used instead, as they are denser and more convenient at almost the same cost.
- **LC**: This stands for lucent connector. This is a smaller version of the standard connector (SC). This supports more ports to be used in the same space. This is probably the most common type used in corporate data centers today and is usually used with SFP (small form-factor pluggable) transceivers.

When choosing which type of cable to implement, the question of distance comes into play. If a customer is looking for great speed with distances over 100 meters, fiber cables are the right choice. What type of terrain will the wiring be subject to? Fiber cables are more protected from outdoor weather than traditional copper cables.
### **Additional Cables**
*Figure. Example of a crossover cable. It looks similar to a UTP cable.*

**Crossover**: Crossover cables are used to connect two computing devices of the same type directly to each other. In computers, this is accomplished via their network interface controllers (NIC) or switches. With a crossover cable, the transmit connector on one end of the wire is connected to the receive connector on the other. These cables are used much less today, as many standards have the built-in capability to try straight through and then crossover if communication does not take place.

**Patch**: Patch cables are used to connect a device to a wall outlet, for example. The wall outlet is wired to another patch panel in the networking closet, and that networking panel is wired into a switch. These cables can also be used to wire servers in a rack to the top-of-rack (ToR) switch. Patch cables look similar to crossover and UTP cables.
### **Networking Standards**
The Institute of Electrical and Electronics Engineers (IEEE) is an association of professional electronic and electrical engineers responsible for many of the standards created in networking today. Founded in 1963, the organization has grown tremendously and has expanded into 160 countries.

The IEEE set out two sets of standards for networking: 802.3 for wired Ethernet networks and 802.11 for wireless networks. Over time, the standards have been changed and updated as technology has evolved. See the following two tables to view these changes.

|**802.3i**|1990|- 10BASE-T: 10 Mbps over UTP|
| :-: | :-: | :- |
|**802.3j**|1993|- Added fiber-optic cable options|
|**802.3u**|1995|<p>- Added 100 Mbps speed, also known as Fast Ethernet</p><p>- Added auto-negotiation of speed (10 Mbps or 100 Mbps)</p>|
|**802.3x**|1997|- Full-duplex (bi-directional communication at the same time—a node could both send and receive traffic instead of one or the other, similar to the difference between a phone and a walkie-talkie)|
|**802.3z**|1998|- 1000BASE-X: 1 Gbps over fiber-optic cables|
|**802.3ab**|1999|- 1000BASE-T: 1 Gbps over UTP|
|**802.3ae**|2002|- 10GBASE-X: 10 Gbps over fiber-optic cables|
|**802.3af**|2003|- Power over Ethernet (PoE), the ability to power a low-power device, 15 watts or less, without plugging it into an electrical outlet, reducing cabling|
|**802.3ak**|2004|- Added support for twinaxial cables, a type of coax with two wires in the cable instead of one; used in short connections (typically just a few meters, such as within a rack) as an inexpensive alternative to fiber-optic cabling|
|**802.3an**|2006|- 10GBASE-T: 10 Gbps over UTP|
|**802.3bm**|2015|- 40 and 100 Gbps over fiber-optic cables|
|**802.3by**|2016|- 25 Gbps over fiber-optic and twinaxial cables|
|**802.3bs**|2017|- 200 Gbps and 400 Gbps over fiber-optic cables|
|**802.3bt**|2018|- Update to power over Ethernet (PoE) to support up to 100W devices|


|**802.11**|1997|<p>- Provides 1 or 2 Mbps transmission in the 2.4 GHz band</p><p>- Uses frequency-hopping spread spectrum (FHSS) (the signal hops between random frequencies to reach the destination)</p><p>- Can also use direct-sequence spread spectrum (DSSS) (data is divided into smaller pieces before being sent with a higher bitrate)</p>|
| :-: | :-: | :- |
|**802.11a**|1999|- Provides up to 54 Mbps in the 5 GHz band|
|**802.11b**|2000|<p>- Provides 11 Mbps in the 2.4 GHz band</p><p>- Uses only direct-sequence spread spectrum (DSSS)</p>|
|**802.11g**|2003|<p>- Used for transmission over short distances</p><p>- Speeds up to 54 Mbps in the 2.4 GHz bands</p>|
|**802.11n**|2007|<p>- Adds multiple-input multiple-output (MIMO) (uses multiple signals on different frequencies to increase the range and bandwidth of wireless networks and forms directed beams toward each client, reducing the interference from other wireless devices nearby)</p><p>- 4–5 times faster than 802.11g</p>|
|**802.11ac**|2013|- Delivers data rates of 433 Mbps per signal or 1.3 Gbps in a three-signal design|
|**802.11ah**|2017|<p>- The first Wi-Fi specification to operate in frequency bands below 1 GHz (900 MHz)</p><p>- Nearly twice the range of other Wi-Fi technologies</p><p>- Can penetrate walls and other barriers better than previous Wi-Fi standards</p><p>- Much lower bandwidth (< 9 Mbps)</p><p>- Designed for Internet of Things (IoT) devices and similar use cases with limited bandwidth needs over larger distances</p>|
|**802.11ax**|2019|<p>- Update to 802.11ac</p><p>- Rebranded to Wi-Fi 6</p><p>- Adds support for 6 GHz frequency range</p><p>- Support for approximately 1–10 Gbps</p>|
One quick note about the frequency used in wireless networking: as a general rule, the higher the frequency, the higher the bandwidth, but the shorter the distance it can travel. In addition, as 2.4 GHz and 5 GHz bands are generally unregulated (any device can use the range of frequencies), other devices may interfere with the signal, slowing down the effective communication rate. This is especially true of the 2.4 GHz range where many devices (microwaves, telephones, baby monitors, etc.) operate.

Also, the bandwidth numbers listed are the theoretical maximum values; they can negotiate to lower values based on the quality of the signal. There is also overhead not taken into account in those theoretical speeds; typically, around half of that value or so will be seen in the real world under good conditions.

Let’s look at how some of these commands such as sudo, ifconfig, route, ping, and nslookup function, using a demo network layout. The sudo command allows for superuser access so that a user can run commands as an administrator. The ifconfig command shows how interfaces are set up in a machine while route defines the paths a data packet takes between networks. We use the ping command to verify connectivity and latency, while nslookup helps resolve a name associated with an IP address.

In this layout, we have two local area networks, or LANs, one in the United States and the other in India, connected through a WAN or wide area network. The LAN in the US comprises two virtual machines, also called VMs – Linux 1 and Linux 2. The third virtual machine – Linux 3 is part of the LAN in India. Let’s now go over the networking commands one by one. As we log in to Linux 1 on our demo network, the fingerprint at the top of the screen shows that we're connected. We also see the last login details above the command prompt.

Let’s now run the ifconfig command. We see two network interfaces. The first one is the regular eth0, and the second is the loopback, or lo, which re-routes data back to itself. The loopback will always have a 127 address, usually 127.0.0.1. The maximum transmission unit or MTU for eth0 shows that we can send a network packet as large as 1460 bytes. The Ethernet IP, or inet address, is 10.128.0.2, the subnet mask is 255.255.255.255, and the broadcast address is 10.128.0.2. Then, we have the IP version 6, or inet6 address followed by the media access control, or MAC address. MAC is a data link layer address as opposed to the IP address, which is a network layer address. Below the MAC address, we see statistics on the number of packets and errors received, RX, and the number of packets and errors transmitted, TX.

Now that you know the IP address of our machine, let's look at routing. Only an administrator can run the route command. Therefore, we need to run the sudo command to run the route command as a root user. The routing table displayed is straightforward, although there could be many more entries. In this table, 10.128.0.1 is the host address, while 0.0.0.0 is the default gateway. The default gateway sends data to the router, 10.128.0.1. Genmask is the subnet mask, which figures out the destination. Here, 0.0.0.0 means the destination address is either unspecified or unknown. Now, let’s reach the next VM in the network. That’s Linux 2 in the United States with the IP address 10.128.0.3. When we ping the IP address, we send Internet Control Message Protocol, or ICMP echo packets to it and receive the returned packets. For each packet sent, the round-trip time, also known as the response time or latency, is a quarter to a third of a millisecond. It's swift because this VM is nearby, compared to the one in India. Our machine will continue to ping until we manually stop it. Before we ping the VM in India, we need to know its IP address. For this, we need to run the nslookup command with the name of the VM, which is Linux-3. On running the nslookup linux-3 command, we first get our Domain Name System, or DNS server and its IP address. The address has a hashtag 53, which is the port for our DNS. The answer to our nslookup is linux-3.c.command-demo.internal. It's the Fully Qualified Domain Name, or FQDN, of the Linux-3 VM, and its IP address is 10.160.0.2. Having found the IP address of Linux-3, we ping 10.160.0.2 thousands of miles away in India. In this case, the round trip takes about 25 milliseconds or about a quarter of a second. That’s about a thousand times longer than the quarter of a millisecond for the round trip between Linux 1 and Linux 2, but it's the speed of light. That’s how long light would take to travel from Linux 1 in the United States to Linux 3 in India and back.

The ping to both Linux 2 and Linux 3 shows there is no packet loss. So the network is functioning normally. We can ping a machine using either its IP address, its short name such as Linux-3, or its FQDN such as linux-3.c.command-demo.internal to get the same results. If we ping a name and encounter a failure, we can fall back on using the IP address. If we succeed with the IP address, it will establish that there is a problem with the DNS, but not with the connectivity. In this demonstration, we’ve seen how to find out the IP address of a machine with ifconfig, the route the data takes by using the route command, how to ping an IP address or hostname, and how to find an IP address from a hostname using nslookup.

In this demonstration, we will see how the traceroute command shows the routers and infrastructure devices on the route from one machine to another. We will also see how to find the owner of a domain by using the whois command.

We’ll use a layout with two local area networks, or LANs, one in the United States and the other in India, connected through a WAN or wide area network for this demonstration. The LAN in the US comprises two virtual machines, also called VMs – Linux 1 and Linux 2. The third virtual machine – Linux 3 is part of the LAN in India.

We can find the route a packet takes from one machine to another by using the traceroute or tracert command, depending on the operating system and its version being used. When we traceroute Linux 3, we see the data goes directly from the local router to the destination in one hop. On the other hand, when we traceroute www.linux.org on the Internet, we see four hops. The first hop was to 216.239.63.174. That router forwarded it to 108.170.243.196, which in turn forwarded it to 141.101.73.2, which ultimately turned up at the WWW host at linux.org at 104.27.166.219. An asterisk in the third hop shows that the router did not respond. Notice that the time for each hop is 10 to 11 milliseconds.

Now, let’s look at how we can find out who owns a domain and more information about that domain by running the whois command. We run whois google.com and get a few screens of information. On scrolling up, we see the domain name is Google.com, and the registrar responding to our whois command is markmonitor.com. MarkMonitor is a company that provides DNS and privacy services. We see its URL here and the dates the registry was updated, created, and when it expires. Of course, Google will renew it much before that date. There’s also a contact email ID and phone number for complaints against abuse. Then, there’s information on what the registrar offers, such as prohibiting client and server deletion, transfer, or update. We also see that Google has four different name servers. As we scroll down, we see some terms of use. The information here tells us that Google is in California, United States. The organization is Google, LLC, and it is a tech organization, again, in California, United States.

Finally, there’s some information on MarkMonitor. If we run whois microsoft.com, we see that the registrar responding to the command is again MarkMonitor. The information displayed is almost the same as that of Google, although there is more information. Its name servers are .msft.net. So, they’re not in the .com domain. The registrant is the domain administrator. The organization is Microsoft Corporation at One Microsoft Way in Redmond, Washington, with zip code, phone, and fax number. There’s also an admin contact email ID, while the tech contact is msnhost.

Having seen who owns two domains, let’s look at a domain that MarkMonitor does not run and do a whois for wgu.edu. Here, you see four DNS servers, and WGU is using awsdns. One of the servers, .co.uk, is in the United Kingdom. There’s also a .net address, a .com address, and a .org address. So, this information tells you how WGU chooses its DNS servers. We also see that it created the DNS domain name on the 17th of April 1997. It was last updated on the 27th of June 2019 and expires on the 31st of July 2022.

So, this is how you would find routers and infrastructure devices on a route by using the traceroute command. And, to find out who owns a DNS domain, you would use the whois command.
### **Basic Network Commands Explained**
Here are some common network commands along with explanations of when and how they are used:
#### **ping**
Ping is one of the most basic tools for testing connectivity to other hosts. It sends an internet control message protocol (ICMP) echo request to a host and listens for the reply. If a reply is received, it will display the time it took and the time to live (TTL) left. Ping has many options for setting attributes of the request, like the maximum TTL, IPv4/IPv6, and the number of requests to send. Ping is useful in troubleshooting connectivity with other devices. If a reply is not received, you will receive a timeout message, which could indicate connectivity issues, firewall issues, or both issues with the other device. In addition, due to the time to get a response, the latency between two devices can be measured, enabling a network engineer to troubleshoot performance problems or a network architect to determine where to place devices to minimize response time to other systems and users.
#### **traceroute/tracert**
Traceroute and tracert are used to trace the route an IP packet takes to a destination. It displays each hop (next router) in a numerical list with the hop’s IP address and the time it takes to receive the packet. The command traceroute is used for Linux systems and tracert is used for Windows systems. It can be useful in determining where a ping fails, troubleshooting performance issues, and other aspects regarding connectivity.
#### **tracepath**
Tracepath is similar to traceroute or tracert in that it displays the path taken by a packet from its source to its destination. Tracepath is useful because it can be used by any user instead of needing superuser privileges. It is primarily used in Linux.
#### **ipconfig**
Ipconfig (internet protocol configuration) provides the user with the IP, subnet mask, and default gateway for each network adapter by default with the /all option information, such as MAC address, DHCP status, and lease information. The command ipconfig/release can be used to release all connections and renew all adapters. It is primarily used in Windows.
#### **ifconfig**
Similar to ipconfig, ifconfig is used to configure the kernel network interfaces. It is implemented at the time of booting to configure the necessary interfaces. Once the interfaces are configured, it is used for debugging or tuning the system. It is primarily used in Linux.
#### **ARP**
ARP (Address Resolution Protocol) displays the IP to physical (MAC) address mappings for hosts that have been discovered in the ARP cache. ARP can be used to add, remove, or modify entries in the ARP cache. The hosts need to be on the local network, as these addresses are discovered by broadcasting to everyone on the network and noting the reply from the owner; broadcast traffic is not allowed through a router so that the system will maintain the MAC address of the router.
#### **netstat**
Netstat (network statistics) displays information about active ports and their state and can be useful in troubleshooting and capacity management. The command netstat -r displays routing information for network adapters. It is available in Windows, MacOS, and Linux.
#### **nslookup**
Nslookup (name server lookup) displays information for displaying DNS information and troubleshooting DNS problems. It is useful in displaying names to IP address mappings.
#### **dig**
Dig (domain information groper) is a command used to query the DNS name servers. It is helpful in troubleshooting DNS problems. It is also used for lookups and will display answers from the query. It is a replacement for nslookup.
#### **whois**
Whois is a tool most often used to look up who owns a domain or block of IP addresses on the internet, including name, email address, and physical address. However, there are many privacy options that hide this information from being returned. It is primarily used in Linux.
#### **route**
Route can be used to display the current route tables on a host. Route can also be used to add or remove routes. This is used by the local host to determine where to send traffic (0.0.0.0 means the default gateway, where the router sends things if it is not otherwise defined in the routing table).
#### **scp**
The SCP (Secure Copy Protocol) command is used to securely copy files between servers, leveraging SSH (secure shell) for authentication and encryption.
#### **ftp**
FTP (file transfer protocol) copies the file from one host to another host. The data is unencrypted. If encryption is needed, FTPS uses SSL/TLS (Secure Sockets Layer, replaced by Transport Layer Security; the same encryption used in https). Transfer uses TCP (transmission control protocol) for reliability and is often used on the internet and other wide-area networks, where errors may be more common.
#### **tftp**
TFTP (trivial file transfer protocol) transfers a file from either a client to a server or from a server to a client using UDP (user datagram protocol) instead of TCP, and so it is usually used on reliable (local) networks.
#### **finger**
Finger displays information about a user or users on a remote system, including things such as last log-in time and username. It is primarily used in Linux.
#### **nmap**
Nmap (Network Mapper) scans networks to see what it can find in terms of hosts and open ports (including well-known ones for many applications). It is commonly used to determine what is deployed on a network for vulnerability analysis, security scans, and related activities. Nmap is not native to either Linux or Windows but can be downloaded for free and used with both.
#### **tcpdump**
Tcpdump displays TCP/IP packets and other network packets that are being transmitted over the network system. It is a form of protocol analyzer (sometimes called a sniffer) and is designed to show the contents of network packets in human-readable form for troubleshooting, security analysis, etc. Tcpdump is not native to either Linux or Windows but can be downloaded for free and used with both.
#### **telnet/ssh**
Telnet and SSH (secure shell) allow a user to manage accounts and devices remotely. The main difference between the two is that SSH is encrypted, and thus all data is secure from eavesdropping, while telnet is unencrypted.

There are different types of networks defined by the distances they cover.

Let's start with the smallest – the Personal Area Network, also called the PAN. As the name suggests, it connects personal devices that are within a distance of 20 to 30 meters. For example, Bluetooth headphones connected to a cell phone form a PAN because you have to be within a certain distance from the phone to be able to use the headphones. Similarly, wireless mice, printers, and keyboards have to be within a certain distance from the computer they are connected to for them to function well. The presence of walls or other things within this area reduces the span of the network.

If you expand the area a little, you have the Local Area Network or the LAN. Your house or an office building are good examples of a LAN. A LAN uses a router or a switch to manage traffic. In a traditional LAN, you use a switch to connect all your devices like servers, workstations, and printers. In a wireless environment, the LAN is called the WLAN or the Wireless Local Area Network. In a WLAN, you use a wireless router to broadcast things over the airwaves and then route that stuff onto various networks.

Now, let's consider a university or any other organization that has multiple buildings spread across a campus of a few acres or maybe a square mile. Every building on the campus, be it the library, finance department, administration, or on-campus housing has a LAN. Some structures may have more than one LAN. You can use a Campus Area Network or a CAN to connect them all. You use a router to connect each LAN to create one campus area network.

What happens when the area extends farther, for example, to a city? To connect an entire city within one network, you can use a Metropolitan Area Network or MAN. In this case, the city will run a high-speed network, and individuals, businesses, universities, government institutions, and other organizations will join this network and communicate at very high speed within that city. Now, let’s take the example of an organization that has offices in different countries across the globe.

In this scenario, you can use a Wide Area Network or a WAN. A WAN, in principle, has no area limits. Even space missions and satellites use a WAN to communicate with each other and command centers on the Earth. The Internet or the World Wide Web is also a WAN that connects different networks, PANs to WANs, across the globe.

There are other types of networks as well, such as a Storage Area Network or SAN. A SAN applies the principle of a LAN but used only for storage. Though there are many other types of networks, these are the major ones that you need to know.
### **PAN**
*Diagram. A Personal Area Network (PAN)*

A personal area network, or PAN, is a network that is centered around a person and their devices. A PAN allows for various devices, such as a person’s desktop computer, laptop, smartphone, tablet, headphones, wireless keyboard and mouse, and speakers to communicate with each other, often over Bluetooth in the case of wireless connections or USB in the case of wired connections. PANs can be used to enable communication between the devices, such as a device sending music to speakers or headphones, or can be used as a jumping-off point for other devices to a network gateway, such as when a mobile phone acts as a Wi-Fi hotspot for other devices.

Apple products are a good example of this. Apple has built their products to integrate together via iCloud. Each device is registered to a single user account. When a user account has multiple devices, notifications, workflow, and data are easily shared across them. For example, when a phone call is being received by an iPhone, it also will alert a MacBook and an iPad that are connected to the PAN. The call is able to be answered by any of the devices.
### **LAN**
*Diagram. A Local Area Network (LAN)*

A local area network, or LAN, consists of computers connected within a limited area. Some examples of LANs are a home, lab, or office building. Most often, LANs use Ethernet, Wi-Fi, or both to connect the network devices.

LANs were popularized in the 1960s for research labs that needed their computers to be connected and able to transfer data at high speeds (at that time). While similar to LANs today, there was significant growth and development through the following decades to LANs. It was not until the 1980s that LANs supported TCP/IP and the commands and programs were often used with it as well as PCs (as opposed to mainframes or minicomputers), which opened the market for even more expansion into LANs as they are known today.

Many LANs are wireless, where users are connected via Wi-Fi and can move unrestricted throughout the coverage area. This is a popular choice for home users and small businesses, as it is easy and inexpensive to install and allows guests to use the network as well.
### **WLAN**
A wireless local area network, or WLAN, is a LAN with wireless connectivity. Unlike LANs, which are wired, WLANs use Wi-Fi to communicate with devices. Users and devices can be placed anywhere and move anywhere in the coverage area. This is a popular choice for small businesses, as it is easy and inexpensive to install and allows guests to use the network as well with a hotspot service. Many WLANs also act as a gateway to the internet for users.

Many private homes use WLAN in the form of Wi-Fi, as it allows for multiple users to be connected to the network (and usually the broader internet), but not be tied down to a specific location in the home.

*Diagram. A Wireless Local Area Network (WLAN)*
### **SAN**
A storage area network, or SAN, is a network that allows access to storage devices specifically instead of the more general networking that can be used for any purpose. SANs allow servers to access devices such as tape libraries and disk arrays while presenting them to the operating system like any other locally attached device.

Typically, a SAN is a network dedicated to storage devices and the servers that need access to them. The key reason for this is to reduce interference from normal LAN traffic during data transfer. SANs may also use other protocols, such as Fibre Channels that do not usually operate on traditional network equipment.

*Diagram. Example of a storage area network (SAN)*

### **CAN**
*Diagram. Campus area network (CAN).*

A campus area network, or CAN, provides networking of multiple LANs across a limited area, like a university campus or a group of buildings owned by a company. Each LAN would typically be constrained to a single building (or even just a part of a building), and the CAN would link them together in much the same way that a WAN (wide area network, which will be defined later) does but over a smaller geographic area. The CAN typically connects LANs owned by a single company, university, government agency, etc.
### **MAN**
*Diagram. Metropolitan area network (MAN).*

A metropolitan area network, or MAN, provides networking across a larger area than a CAN, but smaller than a WAN, such as a whole city or the equivalent of a metropolitan area (hence the name), though it is not necessarily limited by city boundaries. A MAN is made up of many LANs and is owned by many organizations, government entities, etc., within the city to create a fabric of network coverage, often at higher speeds than maybe commercially viable when connecting directly to a WAN when connecting to other entities in the same MAN. Often, this MAN is then connected to a larger WAN (usually the internet) for access beyond the city.
### **WAN**
*Diagram. Wide area network (WAN).*

A wide area network, or WAN, is similar to a LAN, except that it covers a large geographical area within its network. This would be the case for worldwide businesses or government bodies. The internet is an example of a WAN, as it can connect individual users across the globe.

While the technical definition of a WAN is a network spanning a city, countries, or the entire earth, it is easier to think of WANs as a network that connects smaller networks, like LANs. WANs are able to link these smaller networks to transfer data over hundreds of thousands of miles, whereas a true LAN is only able to do so over its small network.

For example, banks use private WANs to connect hundreds of branches across the nation. This allows their sensitive user information to be passed from branch to branch without compromising security or traveling over a public network like the internet. Because of this, a banking customer can go to any branch across a country and have access to their accounts and funds.

In contrast to the above types of networking, there are two other ways of looking at networks: they are defined by who has the resources and who needs access to them.
### **Client-Server**
*Diagram. Client-Server Network*

In a client-server network model, there is a distinct server and a distinct client. The server is the system that stores data and information. The client is the machine that needs access to that data. This is the traditional model of networking since the 1990s.
### **Peer-to-Peer**
*Diagram. Peer-to-peer network.*

In a peer-to-peer model, or P2P, there is no individually designated server or client. Each machine on the network can act as both server and client, sometimes requesting data from other nodes and sometimes answering requests from others. Bitcoin and Tor are examples of peer-to-peer networks.

Several different types of network topologies exist; they include bus, ring, star, and mesh.

The original bus topology consists of one long cable with devices tapped in along the way and a terminator on either end. The terminators absorb the extra signal, preventing it from bouncing back and causing excess noise that would degrade the overall signal quality. The downside of a bus topology is that if the cable gets cut, the terminator disappears, resulting in lots of extra noise, errors, and an overall weak signal. The bus is a commonly used topology in home networks. Some examples include a cable modem plugged into a wireless router or a satellite dish plugged into a television.

A second, less commonly used topology is a ring where each device connects in a circular configuration. For example, A will pass to B, B to C, C to D, and D back to A in a circle. Unlike with a bus, termination is not a consideration here because each device serves as an end and then begins the ring again. Some variations of the ring topology use a dual ring to preserve connectivity. So, for example, if a cut occurs somewhere in the middle of the cable, the signal can go back the other way across the counterrotating ring in the opposite direction. While once used frequently with the Radio Guide, or RG, cable, ring topologies are less common today.

The star is probably the most common topology type in current use. A star consists of a central network device, such as a switch. All devices talk to that switch, making it possible for any device to talk to any other device in the network while the other connected devices are not part of that conversation. A star is an efficient topology that makes running physical cables easier and provides centralized control, especially in a wired network.

The last type of topology is a mesh where every device is connected directly to every other device. Physically, this arrangement is nearly impossible when connecting more than two or three devices because of the excess cables. Logically, a network may use a mesh topology to give every device direct access to every other device even though the physical topology may be a star.

To summarize, the four main types of network topology are bus, ring, star, and mesh. While the star topology is most common, you should know about the general purposes of the other three types because they will come up in various network device discussions.
### **Bus Topology**
*Diagram. A bus network topology.*

A bus network topology is a single line of devices connected together by one shared network cable. Though bus networks are typically drawn as a single straight line, the network cable is rarely as straight. Often the cable is passed around walls or cubicles so that each computer can be attached to the network.

Bus network topologies are uncommon today but were common in the early days of networking, before the advent of the Ethernet hub and switch. In a bus topology, computers connect to the network by physically tapping into the network cable using special adapters. This allows the network cable to remain one continuous conduit while also allowing the computers to send and receive electrical signals on the cable. This leads to an important requirement of bus networks: the ends of the cable must be properly terminated. At both ends of the network cable, special electrical resistors called terminators must be attached to absorb stray electrical signals on the wire. If these terminators are removed or if the network cable is accidentally cut, the electrical signals will not be properly absorbed and will bounce back along the wire, causing communication loss, a condition called signal reflection.

The network in a bus topology is also referred to as a network segment because the network may be extended by adding more segments of cable to either end of the network, allowing more computers to connect to the shared network. This may seem like a good idea but can have significant negative consequences.

Computers on bus networks communicate in half-duplex mode, meaning that you can either send or receive at any given time but cannot send and receive simultaneously. Also, the network cable in a bus topology is a shared communications medium, meaning that all computers attached to the network will receive any and all traffic sent on the network. You can visualize a one-lane road over a bridge, where cars must take turns traveling back and forth. This type of network performs well for a small number of computers but as it becomes more populated, it becomes exponentially more difficult to communicate with each other due to the collisions that occur when two or more computers attempt to transmit at the same time.

To better understand the implications of this shared one-lane medium, consider the following analogy of using walkie-talkie radios. You and your friend both have portable radios that share a common radiofrequency. You can either listen or speak, but not both. Your radio continuously receives whatever signals are sent over the air until you press and hold the transmit button, at which time you no longer hear what is being said on the radio and instead transmit your message. You and your friend may chat back and forth all day without problems, taking turns speaking and listening. Of course, you may have the occasional collision when you both try to talk at the same time, effectively garbling the transmission or cutting each other off. When it happens, you will clumsily have to pause and tell each other to go or wait and then restart your conversation. This is the same behavior that computers face on a shared one-lane medium. They must listen for a clear time to send their data or be faced with a collision that forces everyone to stop sending for a moment while the line clears. Now imagine that you add more people to the walkie-talkie analogy. The more people you have sharing that frequency, the more often you will have a collision and need to restart the conversation. The same is true when adding many computers to a bus network topology: it gets crowded very quickly.

Although a bus topology is very simple and usually inexpensive, overcrowding of devices can make the network unstable or unusable. And if the network cable is broken or cut, the entire network becomes unusable.
### **Ring Topology**
*Diagram: A ring network topology.*

The ring topology was created to combat one of the more challenging aspects of the bus network: traffic collisions. As discussed in the previous section, when traffic collisions occur, all traffic must pause and wait for the line to clear before anyone can send again. This creates delays and degrades the performance of the network. To combat this problem, a ring network topology changes the way that computers know when to transmit and receive.

Imagine sitting around a campfire with children all talking at once. You have a stick in your hand and decide to call it the "talking token." You quiet everyone down and tell them that you will begin passing the talking token around the circle. You explain that only the person holding the talking token will be allowed to speak. Everyone else will listen. After saying a few words, the person with the token must pass it to the next person. No one can hang on to the token for very long. Everyone will have a turn holding the token, even if they have nothing to say to the group. This behavior can be found in computers running ring-based protocols such as token ring or fiber distributed data interface (FDDI). It allows the network cable to remain a shared medium, but it controls traffic.

Interestingly enough, ring topologies do not have to be physically arranged in a circle or even a ringlike shape. The network cable may run in a similar pattern to that of a bus network topology but with some important differences: the network cable is interrupted by each computer on the ring, and the cable is connected back to itself instead of using terminators. In a ring topology, the cable enters a “ring in” port on the network card of the computer and exits a “ring out” port on its way to the next computer in the ring. By definition, a ring is a closed-loop, and the ring topology is no exception. When building a ring topology, even if the computers are all physically arranged in a straight line, the network cable will always connect to itself. The cable exiting the “ring out” port on the last computer will be fed into the “ring in” port on the first computer, thereby closing the loop.

Ring topologies are generally more reliable than bus topologies. However, like a bus network, if the ring is broken, network communication will fail. In cases where the network must be highly available, a dual-ring topology can be used. In this case, there are two sets of cables, and each computer has two network cards, one for each ring. In the event of a single cable break, the second ring can take over, allowing network traffic to continue to flow. However, the dual-ring topology has its greatest benefit in the event that both rings are simultaneously cut. In this case, the two loose ends on either side of the cable break can be connected together, merging the two broken rings into one much larger, but continuous, ring, where traffic can flow. This dual-ring topology is commonly found in fiberoptic networks, such as the synchronous optical network (SONET) ring.
### **Star Topology**
*Diagram. A star network topology with a central switch.*

The star network topology, also known as a hub-and-spoke network, is an improvement upon the bus topology previously described. Unlike the single straight line of the bus topology, a star network is composed of a central network device, such as an Ethernet switch, connected to various network devices, such as servers, computers, and printers, by individual network cables. The name of the topology is derived from its shape in a network diagram. When drawn, the various lines connecting the switch to the network devices appear like the spokes of a wheel or the rays of light emanating from a star in the center.

In contrast to the other network topologies, where a network device such as a computer or printer is directly cabled (and therefore connected) to another adjacent device, in a star network, each device is only connected to the central switch. All device-to-device communication is sent through the switch at the center of the network and then forwarded by the switch to the proper destination.

Today, star networks are the most common type of network found in local area network (LAN) environments. For example, consider the network topology in a typical office where there are computers on each desk and a central wiring closet where servers and network switches may be stored. The desktop computers each have a network card that is connected to a network port on the wall. Behind the wall, the cable continues to run into and through the ceiling to the central wiring closet where all the other network cables in the office have been run. In the wiring closet, these network cables terminate at a patch panel with many network ports, each representing a location in the office where a wall outlet exists.

*Diagram. A patch panel.*

When a computer is moved to a new location in the office, the network administrator can connect the appropriate network ports on the patch panel to a central Ethernet switch. The switch bridges the network traffic together and allows the devices to communicate with one another.

One key advantage of this topology is versatility. Instead of running cables from computer to computer, in a star topology, network cables are often run in the walls to a central closet. In fact, the owner of the building may decide to run many more network cables than are actually needed to anticipate future growth. This gives the network administrator the flexibility to move computers around the office without re-cabling the network. The owner must only connect the computer to the network outlet on the wall and then patch in the corresponding network cable to the central switch. This also means that a break in a single network cable will only impact one network device rather than the entire network. However, if the central switch fails, the network fails, resulting in loss of communication for all the devices in that network.

The star topology is not only used in LAN environments. It is also used in some forms of WAN, where many remote offices are connected to a central headquarters location. Each remote office then becomes a spoke off the hub of the central headquarters. When a computer in one remote office wishes to communicate with a computer in a different remote office, the traffic is passed through the central headquarters network. The star topology eliminates the need for a point-to-point network connection between each remote office.
### **Mesh Topology**
*Diagram. A mesh network topology.*

The term mesh originates from the interconnected threads in a fabric or a net. Mesh topologies are often drawn as a web of direct connections between computers or nodes in a network. However, those connections may be permanent or constructed dynamically, as nodes need to talk to other nodes. A mesh topology permits nodes to communicate with each other; the topology may be either a full mesh, where every node has access to all other nodes, or a partial mesh network, where each node is only able to connect to a subset of the other nodes.

This may have you wondering how you would run cables for such a complex and dynamic network. Unlike other topologies such as bus, ring, or star, mesh topologies are not necessarily constructed using physical network cables. The nodes may connect using Wi-Fi or radio signals or by virtual links such as virtual private networks (VPNs). Another example of a mesh network is a collection of routers that are able to communicate with each other and learn the best path for traffic to take when passing from node to node in the mesh.

Mesh networks are typically used where communication within a network must be highly available and redundancy is needed. The nodes within a mesh network can communicate with each other, and these connections can be changed dynamically if one node were to fail. This behavior is often referred to as a self-healing network because the nodes in the mesh are aware of each other and can establish new connections around failed nodes as needed. Common use cases include wireless networks at home and in the office, as well as large collections of routers, such as on the internet.
### **Centralization**
In the early days of mainframe computers, nearly all computing and network power was centralized in a large data center. Users logged on to machines called dumb terminals to perform their tasks. They were so named because the terminal had no intelligence or sophistication to it. All the programming and functionality were contained in the mainframe; the dumb terminal just accepted user commands and rendered a display with the results. These dumb terminals were placed in offices and cubicles where users needed to work, whereas all the data was safely locked up inside the vault of a data center.

Even though mainframe computers are not used as much today, many resources are still centralized. For instance, strongly centralized web-based applications allow phones, tablets, and computers to behave like dumb terminals. The web browser on these devices does not perform the complex calculations or even store the data that are used within these apps. Instead, the computing and networking resources are hosted in a remote centralized data center, such as a corporate headquarters or a cloud data center. In some ways, cloud computing has allowed a strong centralization approach to make a comeback. You can perform the heavy processing in the cloud without concerns about the hardware of the user’s device.

*Diagram. Centralized network.*

While security is a great benefit of centralization, it is not the only reason to centralize resources. By placing the computing and networking power in a central location, the owners and operators of the applications can better control the performance and availability of the applications. To help explain that point, think about a web-based application that is also available as a locally installed application on your phone. QuickBooks and Office 365 are two good examples, but there are many others. Imagine that you have an older phone, one that has no trouble opening web pages, but other applications just do not run as fast as they used to. You may favor the website version of Office rather than the installed version because it performs better on your phone. Here is what is going on behind the scenes. When accessing Office via the web browser, you are using a centralized web server farm that is running the application for you and just sending back the changes to the display. The performance is also much more constant and reliable, assuming that you have a good internet connection, because the application owners and operators have centralized their resources and can add more resources to their servers during periods of high demand, ultimately keeping the application running well and giving you a positive experience.

Now imagine that instead of using the web-based version of the application, you are using the locally installed version of that application. The performance of the application is based on the processing power of your hardware rather than servers in the cloud. This may allow you to use the application while you are disconnected from the network, but the application owners and operators cannot do much of anything to improve the performance of the application while it runs on your device.
### **Decentralization**
*Diagram. Decentralized network.*

On the opposite side of the spectrum lies decentralization, an approach that puts the computing power in the user’s device rather than a data center. In completely decentralized approaches to computing, there may not be a central data center at all and possibly no need for any of the users to even participate in a network.

Decentralization first became possible in IT when microcomputers were popularized by IBM in the 1980s. These machines could perform tasks without a central computer system to support them, which in turn enabled users to operate autonomously. Soon, challenges began to appear with this decentralized approach. To name just a few:

Users stored data in files on their cassette tapes or floppy disks before hard drives became reasonably priced, which often were not backed up or secured properly.

Mismanaged local security could allow data to fall into the wrong hands or even leave the organization on portable drives.

Data created by one user may have been incompatible with other users due to differences in operating systems, application versions, or even applications themselves, such as WordPerfect versus Word.

The benefits of decentralization can outweigh the disadvantages, though. Decentralized systems are able to operate without a network connection because their data and applications are available locally. This is ideal for portable systems more so than stationary desktop computers. Another advantage of decentralization is the lack of a single point of failure, or, perhaps more accurately, each computer is its own single point of failure because the computers do not rely on each other.
### **Client/Server Model**
*Diagram. Client/Server Model.*

The client/server model is very popular with enterprise applications such as email and databases, but client/server applications can be found in organizations of nearly any size. The name client/server alludes to the shared responsibility of the centralized server and the decentralized client computer that accesses the server. Client/server applications tend to store data in the centralized data center but may leverage the computing power of the user’s client computer to perform some tasks.

This approach offloads some of the computing requirements from the data center’s servers, but more importantly, the client/server model allows application designers to implement advanced user interfaces that would not otherwise be possible in a web-based or terminal-based application. In addition to the user interface, client software, unlike web browsers, usually has access to the hardware of the client computer. Consider the example of a client/server document scanning solution. The server-side software maintains the back-end database and image storage, while the front-end client software performs the scanning and data entry tasks.

There are still challenges with client/server applications. Though the data is stored centrally, the data entry and scanning are performed by the client computer. This could lead to data inconsistency issues if multiple users have the client software installed but they are running different versions of the client software. Additionally, client software is generally created for specific operating system versions, which may complicate future upgrades on the client computers.
### **Peer-to-Peer Model**
*Diagram. Peer-to-peer network.*

Workgroups and very small companies may favor the simplicity of the peer-to-peer model for sharing data as opposed to creating a dedicated server. In a peer-to-peer network, client computers act as both servers and workstations because they share files and printers while allowing a user to log on and use the client computer for normal tasks. For example, consider a small network of four computers. The users have a collection of files they have been sharing via USB drive. The process of copying and pasting the files has become cumbersome and introduces delays in their work. They could, instead, choose to store the files on one computer and share them over the network in a peer-to-peer fashion.

Creating a peer-to-peer network is very easy. Most operating systems simplify this process by automatically discovering other computers with shared resources. In many cases, you can even automatically discover resources on different operating systems, which can be a big help if you have a mixed network of Microsoft Windows, Apple MacOS, and Linux.

Remember, the computer that will serve the files will perform double duty as a workstation and a server. When choosing the computer that will serve the files, select the least active computer or the most powerful computer in the group. This will minimize the performance impact of other computers accessing files on that computer.
### **Wired versus Wireless Networks**
*Diagram. Wired and wireless networks.*

It is important to note that the network architecture does not depend on whether your network is wired, wireless, or a mixture of the two. You can operate a client/server network using wireless networking just as easily as you can with wired networks. The difference comes down to a couple of factors: portability versus stability.

Wireless networks are great for portability. You can take the computer wherever it is needed, as long as you have a wireless signal. The challenge is the strength and stability of that wireless signal. In some buildings, wireless signals may be intermittent, or you may encounter dead spots where the signal does not exist.

This is where wired networks excel. While you do not nearly have the degree of portability while connected to a wired network, you will have a very stable connection. In some cases, the bandwidth of a wired network may also exceed the bandwidth of the wireless network. However, take care not to assume that one connection type will always be faster. Wireless networking technology is constantly improving and may exceed the speed of some wired networks.

The term virtualization is often associated with virtual machines (VM) operating as servers within a data center, but it can include many different types of hardware: servers, workstations, storage, and even network devices. As a term, virtualization does not describe a particular technology or product. Instead, it describes the technique of converting a hardware-based resource into software.
### **Virtualization**
Virtualization has been used in IT since the 1970s, when large mainframes and minicomputers needed to be logically divided into smaller virtualized computers to run workloads in isolation. Today, virtualization fulfills similar purposes, such as allowing one physical server to host many different virtual machines or to create network devices on demand, such as virtual routers and firewalls.

Virtual devices operate similarly to their physical counterparts but with a few differences and benefits. The very nature of these virtual devices practically gives administrators superpowers in the data center. Tasks that would be challenging or even impossible with physical devices now become commonplace, thanks to virtualization. For instance, you can do the following:

- deploy new virtual devices quickly and on demand without the need to physically install new hardware move virtual devices within a data center or even between data centers without shipping or moving physical equipment
- increase the reliability of an application or service by virtualizing it and separating it from hardware that may have once been a single point of failure (SPoF)
- create point-in-time snapshots or clones of virtual devices for backup and recovery purposes
- increase or decrease the compute, storage, and network resources allocated to a virtual device on demand as its utilization rises and falls

Computers use one or more servers to perform different tasks. Twenty or so years ago, applications ran on servers that were each connected to a computer. Because various applications sometimes didn't function well together, adding new applications often meant adding new servers. In addition to the cost of buying servers, wasted resources were another disadvantage of this method as most servers only operated at 5% to 15% utilization.

The advent of virtual computing solves the problem of multiple servers because it enables a personal computer with a single server, such as a regular x86 type, to run several virtual machines or VMs. Virtual computing allows the user to access applications that are not on-premise via the internet through a server. Think of it as a modern iteration of the old mainframe concept. It requires a computer, hardware, software, and an internet connection.

Virtual computing starts with a physical computer or server with a chassis containing the motherboard, the central processing unit or CPU, memory, disk drives, network cards, and so forth. An operating system, like Windows, Linux, or Mac OS, runs on top of this physical infrastructure, and various applications run on top of that. Modern servers built for virtualization are similar to those used in the past, although with more CPU and memory. A single server running virtualization might require 500 gigabytes or one or more terabytes of Random Access Memory or RAM. Storage might also be external, on a SAN or Storage Area Network with many dedicated storage devices. However, at a high level, servers are similar to what we used back in the day.

To connect with and host multiple VMs on a physical computer, you require a hypervisor. Some hypervisors are proprietary, and some are open-source. Common examples include ESXi by VMWare, Hyper-V by Microsoft, KVM by Linux, and XenServer by Citrix. On top of the hypervisor, VMs utilize virtual hardware. Just as a physical machine has CPU, memory, disk drives, networks, each VM uses a portion of the physical device from a server. For example, if a physical server has 20 CPUs, VM "A" might get two CPUs while VM "B" gets four CPUs. In another case, a physical server with 128 gigabytes of random-access memory or RAM might give 16 gigs to the VM "C" and 32 to the VM “D”— and so on through all the various components.

With virtual computing, an operating system runs on top of virtual hardware, just like an operating system runs on top of physical hardware. The operating system may be the same as or different from the underlying hypervisor. For example, the host computer can run Hyper-V, which runs on Microsoft Windows, while the VMs might be running Windows or different versions of Linux.

And finally, various applications run on top of the operating system, similar to a physical environment.

To summarize, virtual computing is similar to physical computing in that it requires a computer, hardware, an operating system, and applications. Instead of using multiple physical servers to run individual tasks, virtual computing uses a hypervisor via a network connection to run many virtual machines.
### **Hypervisors**
Virtualization relies on a special type of software, known as a hypervisor, which creates the virtual hardware for devices. There are many different varieties of hypervisors, including open source and commercial. Some hypervisors run as standalone applications on Windows, MacOS, or Linux, whereas other hypervisors are installed as the underlying operating system itself. Regardless, all hypervisors have something in common: they use software to create the illusion of physical hardware.

Hardware is essentially programming instructions baked into copper and silicon chips. With enough skill, you could recreate that programming in software. Now imagine that you learn how to recreate all the hardware in a computer’s motherboard as a series of software programs. You continue the process with its graphics card, network card, and even its hard drive. You now have a collection of programs that, when connected together, have the appearance of a physical computer, but it is executing as software. This is virtualization, and you have just created your first set of virtual hardware. Each instance of virtual hardware is called a virtual machine, or VM.

The virtual hardware is such an elaborate and convincing illusion that you can actually install a real operating system within the VM, and it works as if it had been installed on physical hardware. The operating system (OS) installed within the VM is often referred to as a guest OS to differentiate it from the operating system of the underlying physical computer, which is called the host OS. To help you remember these terms, just think of the hypervisor as a host to a number of house guests called VMs.

The hypervisor provides the VM, and therefore the guest OS, access to the physical CPU and RAM resources as well as access to a virtual disk that the guest OS believes is a real, physical hard drive. Optionally, you may also provide a virtual network card that also appears real and that will give the VM access to the network. With few exceptions, the guest OS will behave in the same way it would if it had been installed directly on physical hardware. To make the illusion complete, some hypervisors include special device drivers that you can install within the guest OS to gain performance improvements and access to features that would be otherwise impossible on physical hardware.

Hypervisors are able to host multiple VMs, each running its own guest OS and applications. Each VM has its own virtual hardware that the hypervisor manages and keeps isolated and independent of the other VMs. The hypervisor serves as a resource traffic cop in that it manages how each VM accesses and consumes the physical hardware resources, such as CPU, RAM, networking, and storage. To the rest of the world, these VMs appear as isolated and independent as individual physical servers. Also, despite the magic of virtualization, VMs still cannot communicate telepathically with one another. So, if you want them to talk to each other, you will have to install virtual network adapters in the VMs and configure their networking as you would do with your physical servers.

There are two types of hypervisors: Type 1 (bare metal) and Type 2 (hosted). Both types can host VMs; however, they have very different use cases. Type 2 hypervisors, which you will learn about first, look and feel like any other application that you may run on your laptop. Type 1 hypervisors typically requires dedicated hardware and are installed as that machine’s operating system, making them more commonly found in data centers than in home networks.

*Diagram. Type 1 and Type 2 hypervisors.*
#### **Type 1 Hypervisors**
Type 1 hypervisors, also called bare- metal hypervisors, differ from Type 2 hypervisors in the way they are installed. Remember that a Type 2 hypervisor, such as VMware Workstation, is installed as an application within your existing computer or laptop operating system. Type 1 hypervisors, such as VMware ESXi or the open-source KVM hypervisor, are operating systems that natively run virtual machines and are intended to be installed on a dedicated bare metal server. This is a significant difference because, as an operating system, the Type 1 hypervisor has complete access to the underlying hardware of the physical computer, which alleviates the performance penalty that Type 2 hypervisors commonly face.

Another important difference between the hypervisors is in how you access and manage the host and the guests. Type 1 hypervisors present very little information to the console screen of the host computer. While there may be a limited user interface for troubleshooting the hypervisor directly, all administration is typically performed from another computer, such as your laptop. On your laptop, you would direct a web browser to the IP address or hostname of the hypervisor to access its web-based administration portal.

To make this a bit clearer, consider the earlier example of using VMware Workstation to run a Windows Server VM on your laptop. The hypervisor was just an application installed on your Windows 10 laptop that you were logged in to and that was able to directly interact with the settings of Windows 10 (your laptop), VMware Workstation (the hypervisor), and Windows Server (the VM). Conversely, if you have installed a Type 1 hypervisor on a computer, you can see the basic troubleshooting interface on its monitor but must use a web browser on a separate computer to administer the hypervisor and the VMs running on it.

Also, remember that VMs are typically connected to the network just like any other physical computer. Users that need to connect to a VM typically are not granted access to the hypervisor configuration portal; rather, they are given an IP address or hostname for the VM that they need to connect to.
#### **Type 2 Hypervisors**
A Type 2 hypervisor, sometimes referred to as a hosted hypervisor, is installed as an application on personal computers or laptops. This allows the user to run different VMs that all share the laptop or computer’s hardware resources. This allows you to run operating systems within a VM other than what was natively installed on your computer or laptop. For instance, imagine that you are an application developer creating a new application that will run on Windows Server. Your laptop is running Windows 10, which is similar to Windows Server but may not be close enough for you to properly test your application. You could install a Type 2 hypervisor, such as VMware Workstation or Microsoft Hyper-V, to allow you to run VMs on your laptop. This would allow you to create a VM running Windows Server and then log in to the VM to install and test your application.

In another example, you are a student with a laptop running MacOS. One of your classes requires you to gain familiarity with Linux, but you do not want to purchase a second computer. Again, you could install a Type 2 hypervisor for MacOS, such as VMware Fusion or Parallels Desktop, which would allow you to create and run a Linux VM on your laptop.

One important aspect of Type 2 hypervisors is that they are installed and run as a regular application on your computer or laptop. This means two things: first, the hypervisor is competing for hardware resources with all the other running applications on your computer; and second, the hypervisor does not have direct and unrestricted access to the physical hardware but instead must send all your VM’s hardware requests through your computer’s operating system. Both issues can lead to degraded performance within your VMs, but slower performance may be far more desirable than buying and carrying around separate computers.
### **Cloud Computing**
*Diagram. Cloud computing.*

Cloud computing may be the most commonly used term in IT, but if you ask an IT professional to define it, you will likely get the response, “It depends.” Pinpointing a single definition for cloud computing is difficult because it is many things to many people. Instead of trying to find one all-encompassing definition, consider the meaning and business implications for some of the more well-known and accepted characteristics of cloud computing: on-demand, self-service, resource pooling, elastic, accessible, and measurable.

Business owners and executives do not usually make impulse decisions about strategic direction, mergers and acquisitions, or massive marketing campaigns, but it can seem that way if IT is left out of the decision-making process. Implementing change takes time, and in the business world, time can cost you opportunities. Therefore, a quick reaction time within IT is critical to the success of the business. Without cloud computing, it may take weeks to acquire the necessary hardware, software, and manpower to satisfy the new business needs. The on-demand nature of cloud computing provides IT professionals the means to provision new servers, applications, and other resources in a self-service manner, often within minutes of the request.

Cloud providers, such as Amazon (AWS), Google (GCP), and Microsoft (Azure), purchase and manage vast quantities of compute, storage, and networking resources around the world. In doing so, they are able to pool these resources to achieve economies of scale, which can improve pricing and provide additional capacity to their customers for sudden bursts in their businesses. It is in this elasticity where cloud computing proves to be an immense business asset. Consider that you run a retail business with an e-commerce website. Most of the year, your website traffic is predictably average, but during the three months of the holiday shopping season, your website receives so much traffic that it occasionally crashes or performs so poorly that customers go elsewhere. What you need is a data center that can dynamically increase in size during the busy months and then shrink back down to size during the rest of the year. This elasticity, often referred to as scalability, is yet another key benefit of cloud providers. During your busy months, you could leverage the resources of a cloud provider to keep up with customer demand and then recede back to your own data center during the rest of the year.

Another reason cloud computing is so popular is the global reach that it provides companies. Consider again the retail business example. Imagine the business is located in a small town in the middle of the United States, but its products and services attract a global customer base. Currently, the business’s website is hosted in one location in the United States, which is great for U.S. customers, but the site is slow when accessed from Europe or Asia. To counter this problem, the company could leverage the global presence of cloud providers to host their website in data centers around the world. This improves their customer’s experience, which in turn may improve sales.

Finally, services in the cloud must be measurable. This alludes to the way that you pay for services in the cloud. Much like the way you pay for electricity or water at your home, cloud providers tend to charge you only for what you consume. This could be the number of hours that your virtual machines are running, the amount of data storage you have consumed, or even the number of gigabytes of network bandwidth that your customers consumed while browsing your website. So, just as you turn the lights off as you leave a room, do the same in the cloud. When not using resources, such as test or development systems, power them down and remove extra and unnecessary data. Of course, if you tend to leave the lights on, they will stay on, but you can expect a hefty bill at the end of the month. Ultimately, if you are conscientious about how you consume cloud resources, you will likely find cloud computing to be a very affordable alternative to traditional data centers.

Products and services within cloud computing are organized based on the type of service being offered and by the location where the service is hosted. The various types of cloud services are categorized by the cloud service models, whereas the hosting locations are classified by the cloud deployment models.

There are several models or forms in Cloud Computing. One of them is Infrastructure as a Service, commonly known as IaaS. In this model, the cloud service provider manages the underlying infrastructure, including servers, networking, and storage, as well as access to the services and firewalls. Organizations subscribing to the service then deploy virtual machines, middleware, and applications, just as you would with on-premise virtual machines. Moving on-premise virtual machines into the cloud with similar operating models is often called the lift-and-shift approach. While this model does not leverage all the capabilities that a cloud has to offer, many organizations opt for it because it is simple. Organizations get full control of the platform, allowing them to choose the operating systems and software they want to install, make back ups, or run patches

Another model is Platform as a Service or PaaS. In this model, the service provider offers the platform on which you build the code instead of hosting a virtual machine in the cloud. You select a language such as Java, Python, or .Net and focus on the tasks that add value to the business, which is writing the code. The cloud provider manages the underlying infrastructure, firewalls, and networking, as in the IaaS model. Besides, the provider also chooses the kind of virtual machines on which the code will run and, how these machines will be controlled. The service includes selecting the operating system, when and how to update, run, and handle the patches, and ensuring that everything is running and operational.

Yet another cloud computing model is Software as a Service or SaaS. In this model, the cloud service provider runs the entire stack of physical infrastructure, virtual machines, operating systems, middleware, and applications. Examples of SaaS which we use daily are Gmail, Outlook.com, and Salesforce.com. You can use these services, but cannot control their features. You cannot tell these providers that the algorithm needs to be checked and ask for control over it. You can post feedback and suggestions for feature additions and modifications. However, the prerogative to accept and implement your recommendations is that of the service provider.

While these are the three most recognized models, a fourth, called XaaS, offers varied services. Here, the X can represent anything from a directory service to a backup or a database. In the case of Database as a Service or DBaaS, the cloud service provider will run the database for you, including taking care of underlying factors like updating the database and operating system. This service leaves you free to focus on its content and security. Most services in cloud computing will fall into one of the three forms that we have discussed. While a vast majority will fall in the first two categories, some offerings like G-Suite by Google and Office 365 by Microsoft will fit in the third.
### **Cloud Service Models**
*Table. Comparison of cloud service models.*Table Description

There are three primary service models within cloud computing: infrastructure as a service (IaaS), platform as a service (PaaS), and software as a service (SaaS). However, the industry also includes the catch-all term X as a service (XaaS), where X could be nearly anything from security, accounting, hardware, etc. This structure is based on the shared responsibility model. Each of these models is a classification of responsibilities. This means that in each model, the consumer and the provider are responsible for different aspects of the service offering; however, note that in all cases, the customer remains responsible for, and the owner of, the data.
#### **IaaS**
You will start by learning about IaaS because it bears the closest resemblance to a corporate data center. First, you must understand the word infrastructure. In this context, infrastructure refers to the physical servers, storage, and networking that is required to exist before you can create any virtual servers or install any applications. This underlying infrastructure is maintained by the cloud provider so that you may focus your attention on the virtual machines and your applications. The cornerstones of the IaaS offering are server virtualization, storage virtualization, and network virtualization. Infrastructure as a service is very customizable, but it also means you are responsible for managing more things, such as the virtual machine configuration, its operating system, and all the patches that it requires.

With regard to server virtualization, the cloud provider manages the hypervisor and the underlying hardware. You, as the consumer, are given access to a web portal, which enables you to create virtual machines and customize their resources to fit your business and application needs. The cloud provider allows you to choose the number of CPUs, the amount of RAM, the amount of storage, and even the number of network cards in the VM.

Storage virtualization helps cloud providers offer you storage solutions that can expand in size or change in performance based on your business needs. When creating your VMs, you may be given the choice to provision regular storage at the normal price or very fast storage at a premium price. Your selection will depend on your application's requirements.

Network virtualization gives cloud providers the ability to create virtual private cloud (VPC) networks for each of their customers, keeping each VPC network isolated from the others. As a consumer, you may even create additional networks to isolate your virtual machines for testing or development purposes. Your VMs will use the VPC network to communicate with each other just like the physical computers in your data center use your internal network to communicate with each other. The VPC network may also have routers and firewalls that allow your VMs access to the internet or to other data centers around the world.

Server virtualization continues to play an important role in cloud computing, both for private clouds hosted within corporate data centers and for public clouds such as Google, Microsoft, and Amazon. Not only are virtual servers the flagship product offering in the infrastructure as a service (IaaS) cloud service model, virtual servers are also used by cloud providers behind the scenes to deliver their customer-facing platform as a service (PaaS) and software as a service (SaaS) products.

A word of caution is warranted here. The cloud provider typically has little to no interaction with your virtual machines, leaving you with the responsibility to protect them from disaster (via replication) and data loss (via regular backups). In the event that one of the cloud provider's physical servers were to fail, the cloud provider is responsible for repairing their equipment; however, depending on your support agreement with the cloud provider, they may or may not be financially liable to you for the outage or responsible for restarting your virtual server following the outage.

Also, keep in mind that in most legal jurisdictions, the cloud provider is not obligated to automatically back up your data, keep archive copies of your data, or even move your data to another location without your express consent and direction. This is not much different than deploying a VM in your own home lab environment. It is not automatically backed up or replicated to another location. If you suffer a hardware failure, you lose the VM. The principle advantages of IaaS solutions are that you can customize the infrastructure to suit your needs, install nearly any application in your VMs, and, if desired, implement high availability and backups for your solution. When deploying a production application to the cloud, build redundancy and disaster recovery into your design by deploying at least two of everything: one instance in a nearby data center for production use, and one instance in a geographically distant data center for safekeeping.
#### **PaaS**
If you just need a platform on which to deploy your application or you simply need a database without the hassle of managing the server, PaaS makes this easy. In PaaS solutions, the cloud provider is responsible for the virtual servers and, in some cases, the services that run on top of them, such as a database engine, and provides you with a platform on which you can run your code or store your data.

You may have seen or heard about PaaS services in the past and yet not realized it because cloud providers rarely label their services as IaaS, PaaS, or SaaS. One example of a PaaS offering is basic web hosting, a service that has been around since the dawn of the web. Web hosting, like all PaaS offerings, is a partially managed service. The provider gives you limited control over a web server that they maintain for you. You have the ability to log in and upload your code, but they maintain the server for you, including the virtual hardware, guest OS, web services, and the patching for the OS and web services.

PaaS solutions are particularly attractive for application developers because it allows them to deploy their code to an application runtime environment, such as Java, .NET, or Node.js, that is backed by an ambiguous pool of compute resources. That may sound magical, but the cloud provider's goal is to abstract the details of the resource consumption away from the application code and allow the application to scale up and down based on its load, such as the number of people using the application, or the amount of data the application is processing.

A branch of PaaS solutions designed to further simplify the deployment of application code has gained so much popularity that it has been given its own name: serverless computing. The term serverless is not intended to imply that there are no servers, but rather that the consumer of the service (the developer in this case) typically is not even made aware of the number of servers being used in the back end to run the application. These solutions are also gaining in popularity because they can quickly deploy and then scale new applications with minimal effort required by the IT operations team, thereby increasing the reaction time of IT to changing business needs.
#### **SaaS**
SaaS solutions, like PaaS solutions, are often overlooked because they are so ubiquitous. The term software in SaaS could represent nearly anything you consume over the internet. A few examples include social media (Facebook), word processing (Office 365), and a line of business applications (Salesforce). Even now, you are using an SaaS solution to view this course and read this text. Software as a service allows consumers to store and potentially publish information without the need to manage the underlying applications or infrastructure.
### **Cloud Deployment Models**
To make things simpler, the service models discussed previously were all presented as being offered from a public cloud provider, such as Amazon Web Services or Google Cloud Platform. You may see these same service models inside corporate data centers as well but deployed as private cloud solutions.

There are several different cloud deployment models. Although it may seem like the differentiation is location, it is actually similar to the service models discussed above. The differentiating factor is responsibility. The figure shown here depicts the different cloud deployment models.
#### **Private Cloud**
The term private cloud is most often associated with equipment hosted within a single company’s on-premises data center. The company purchases or leases the computer, storage, and networking hardware and maintains the data center facilities. If a failure occurs, the company is responsible for repairing the problem themselves because all the equipment belongs to and is managed by them.

It is possible to have a private cloud that is not hosted within the company’s data center. The company may lease space in a commercial data center and operate the equipment there. This is usually referred to as co-locating, or a "co-lo" solution, because the company is co-locating its equipment with the commercial data center’s equipment. Some public cloud providers also offer this as an option for customers who require complete control over the physical equipment yet wish to house the equipment outside their own data center.

The primary advantage of a private cloud is the ownership and control that a company has over the equipment. Some companies use a private cloud only because of regulatory restrictions that mandate the company to maintain absolute control over the hardware and software that run a particular system.
#### **Public Cloud**
There are many public cloud providers, but some providers, such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP), are more well-known because of their marketing and experience in the industry. As with "private cloud," the term "public cloud" refers to the ownership and maintenance of the underlying infrastructure and facilities. In the case of a public cloud, the cloud provider is responsible for maintaining the hardware and repairing the infrastructure instead of the customer.

Another common misconception is that public cloud providers are inherently insecure because they host content from many different companies; however, the opposite is much more likely to be true. Public cloud providers must keep their customers’ data isolated from one another and must follow strict rules that govern how data is handled and destroyed. Even their physical data centers are required to have the most state-of-the-art security available. In some cases, public cloud providers may have a higher level of security than the customers they host.

Another consideration is the concept of multi-tenancy, meaning when multiple tenants share the same physical resource. In any multi-tenant situation, there are potential security and performance implications because you are sharing hardware with other companies, though the risks may not be as significant as they seem. Multi-tenancy in computers is analogous to living in an apartment building. Many different people live within the same apartment building (the physical computer), but each tenant (the customer) has their own apartment (a VPC) with a lock on the front door (a firewall). In addition, for extra privacy, the members of a tenant’s family (the servers of one customer) may even have their own rooms with locked doors to further isolate themselves.
#### **Community Cloud**
Community clouds are uncommon in the commercial sector but may be found in universities or government agencies. These clouds are data centers that are jointly owned and operated by the tenants. Think of it as a semi-private cloud for each tenant, with the added benefit that the maintenance and management of the underlying infrastructure is shared by all the tenants.
#### **Hybrid Cloud**
The term hybrid cloud refers to a combination of private cloud and public cloud and is most commonly associated with companies that extend their applications and services between their own data center and that of a public cloud provider's. This may be done to allow the company easy access to additional computing resources during times of burst demand, or as a way to host most of their services in the public cloud with the exception of the few applications that are subject to regulatory controls and must remain at an on-premises data center.

For hybrid cloud computing, the customer will require a dedicated connection between their on-premises data center and the public cloud provider. This connection may be a virtual private network (VPN) established over the internet or a dedicated wide area network (WAN) connection maintained by a telecommunications provider.
#### **Multi-Cloud**
Multi-cloud is the concept of leveraging the services of multiple public cloud providers, such as hosting your website at AWS and GCP and balancing the users between these providers. This concept, in practice, can add redundancy and flexibility.

Another use case is when a company currently uses services in one cloud provider but another cloud provider excels or provides cheaper services in a particular area, for example, machine learning or data warehousing.
### **IT Security Fundamentals**
There are several key terms that IT professionals use to describe different aspects of information security, or InfoSec. The IT professionals within InfoSec practice a discipline called security operations, or SecOps. Pay careful attention to the definitions of these terms because the SecOps definition may be very different than what you have used in other contexts.

Organizations are likely to face several threats to their networks. These could come from rogue administrators, information or intellectual property theft, ransomware, hackers, and access control between two sets of equipment like sales and internal engineering. The security team needs to brainstorm, list all possible risks, identify the most likely kind of attack, and formulate a strategy to handle it.

A network-based attack could originate from insecure passwords. A hacker can use one of the password dictionaries available on the Internet to access your virtual machine. There are several instances of hackers accessing sensitive information and putting it online, making organizations vulnerable to lawsuits. A recent example is that of someone in a credit monitoring company using “admin” as both the username and password used to secure sensitive information. A hacker guessed this username and password combination and leaked the information online, creating a public relations crisis for the company.

Social engineering attacks are another threat. These can happen when you receive an email warning you of having exceeded your credit limit or informing you of an offer that you can redeem. In response, you click a link that displays a web page resembling a genuine one, like that of your bank. Hackers create websites that look exactly like genuine ones. Misspelling the name of your bank in a URL can also redirect you to a hacker’s site that can steal all your sensitive account information. Social engineering attacks occur due to human vulnerabilities. Users often misread malware or ransomware attacks as those from genuine sources. Therefore, it is essential to educate them about such security risks.

There could also be problems with firewalls not being patched on time. Firewall vendors often release security updates to plug vulnerabilities. It is best if you run the patch immediately to be ahead of potential attackers who might try to exploit these vulnerabilities.

Smart attackers might also try to steal communication from a company to learn typical wording of internal authorizations For example, an attacker could word communication to the accounts department to make it look as if it is from the president of the company requesting a check for $10 million to close a big deal. You require a clear mitigation strategy to handle these risks. For example, you can educate users about the risk of clicking the wrong links or the importance of patching software vulnerabilities on time. A company can also create a policy for the accounting department to check with the president in person before issuing a check over a trigger amount, to ensure the request is genuine.

The attackers and security teams always seem to be playing cat and mouse games. The security team tries to build new defenses, while the attackers try to get around them. That is why the security team must always be on top of their game.
### **Network Security Terminology**
Many of these terms are defined using the other terms. Where applicable, the corresponding terms are emphasized in the definitions below.

*Asset*: A person, device, location, or information that SecOps aims to protect from attack.

*Attack*: An action taken by a threat that exploits a vulnerability that attempts to either block authorized access to an asset, or to gain unauthorized access to an asset.

*Risk*: The potential of a threat to exploit a vulnerability via an attack.

*SecOps*: The abbreviation for IT security operations; a discipline within IT responsible for protecting assets by reducing the risk of attacks.

*Threat*: Something or someone that can exploit a vulnerability to attack an asset.

*Vulnerability*: A weakness in software, hardware, facilities, or humans that can be exploited by a threat.

Vulnerabilities come in many forms, from hardware and software to buildings and people. You could have an application that runs outdated code that could be exploited by a threat, such as a hacker or a virus. Or your data center may be vulnerable to physical break-ins because it has windows or doors that are left unlocked. Humans can also become vulnerabilities by falling prey to trust games designed by attackers to gain unauthorized access to a building, computer, or network. The goal of SecOps is to eliminate the vulnerabilities where possible, but that is not always possible. You may, however, be able to mitigate the impact of the vulnerability by reducing the amount of damage a potential attack could have on your organization.

A vulnerability scanner is an indispensable tool for detecting vulnerabilities within servers, computers, and network devices. Vulnerability scanners can be cloud-based or may be installed as a software application on your laptop or a dedicated security server. It is important to regularly run these scanners within your network for early identification and mitigation of new vulnerabilities.

Next are some common vulnerabilities and how they could impact the security of a network.
### **Poor Physical Security Measures**
The best electronic security in the world will not help you if your physical security is weak. Even the best encryption on the market today can be eventually overcome through sheer brute force methods of trying key after key until one finally unlocks your secrets. Yet SecOps teams still strongly urge their users to encrypt their data. The reason? It is all about time. The SecOps team, or more accurately, software the SecOps team uses, frequently replaces all the encryption keys on the network. This means that even if you captured and then cracked one encryption key, potentially within days that key will no longer be in use and therefore will not unlock the data.

However, there is a loophole in this strategy: theft. If attackers gain access to your files or to your physical computers, they can simply steal a copy of the data. The SecOps team cannot change the encryption key if the copy leaves the network, which means the attackers now have time on their side. Though it may take months or even years to crack the encryption through brute force, it will yield eventually. The moral of the story is that you need to protect your assets with both physical and electronic security measures.
### **Weak Passwords or Using Default Passwords**
Short, simple passwords are easy for attackers to compromise either by guessing them or by breaking them using brute force tools that leverage dictionaries of common words and letter combinations. You can combat this by using long, complex passwords that include upper and lowercase letters, numbers, and symbols, but these passwords are difficult to memorize. The irony is that increasing password complexity requirements may lead to more trouble because some users will just write down the complex passwords rather than memorize them.

Another challenge is relying on default passwords instead of changing them. It is very easy to find the default password for most devices using a simple internet search. If you install a new device and leave the password at its default value, you have essentially left your system unlocked. While compromising a server is bad, consider the amount of visibility and control that an attacker would have if they gained access to your switches, wireless access points, routers, and firewalls.
### **Misconfigured Firewall Rules**
Again, consider that your firewall had a default password and an attacker has just gained access to it. The attacker could easily change the firewall configuration to allow access into your network without your knowledge. This is a bit like leaving a key on your doorstep, in plain sight. An attacker could come along, make a copy of the key, and secretly enter your locked home without leaving a trace behind.

Configuration mistakes are equally dangerous to your security and can also leave holes open for would-be attackers. It is a good policy to regularly review your firewall configuration, identify what each rule does, remove any rules that are no longer needed, and regularly change the password.
### **Personal Devices Within the Network**
Many businesses are adopting a mobile computing strategy that often includes a concept known as bring your own device (BYOD), which encourages users to bring their personal computers, tablets, or phones to work to use on the network. Allowing users to bring their own computer may save a company money at first, but extreme care must be taken to ensure those devices do not contaminate the network with viruses or malware they picked up elsewhere. For instance, imagine you have a laptop computer that you use at home to watch cat videos and shop online. Unbeknownst to you, one of those websites was actually compromised, and that cat video just brought home a nasty virus to your computer. Without realizing you were doing any harm, the next morning you take your laptop to work and proceed to infect the entire network. This sort of attack is known as a Trojan horse because, like its historical namesake, the Trojan horse unleashes a multitude of viruses, malware, or ransomware into a network from one seemingly innocuous computer.

While that may seem unlikely, it happens often enough that some corporations require their workers to install special approved antivirus and anti-malware software on their devices before connecting to the corporate network.
### **Advanced Persistent Threats**
Using the earlier analogy from the cat video virus, consider now that the virus actually did not do anything. Or so it seems. In reality, it is lying in wait for some preordained time or a trigger from an outside source, at which point it will unleash its attack. This is known as an advanced persistent threat (APT) because the virus or malware remains undetected while it sits idle for long periods, perhaps months, just waiting for the right time. The real threat here would be ransomware or other highly destructive malware, which, once activated, causes so much damage that the IT department must restore from backups. But what if the virus infected the backups? That is exactly what an APT does. It sits idle long enough to ensure that it has been added to as many backups as possible so that even if you restore from backup, you have no choice but to comply with the attacker.
### **Zero-Day: Protecting Against the Unknown**
Thus far the impact and mitigation of vulnerabilities that are known to the public have been discussed, but what about those that are just discovered, or those that were discovered but never reported? You now enter the world of the zero-day exploits. The term zero-day alludes to the fact that the exploit or vulnerability is not yet known by the public, meaning there is no patch available to mitigate this vulnerability.

Unfortunately, not all security analysts work for the good guys. Some people are paid to discover vulnerabilities in software, and then report them only to illicit organizations that will then use them to attack their targets. Zero-day exploits are particularly dangerous because no pattern files exist in your antivirus or anti-malware software.

The good news is that there is hope. Some intrusion prevention system (IPS) vendors include an option that allows administrators to forward unknown or suspicious code patterns to the vendor for analysis. If enabled, the IPS or firewall device blocks the unknown code the first time it sees it but may permit the code to pass later if the security vendor determines that it is not a risk. Of course, not all firewall and IPS vendors provide this level of security. In those cases, the customer remains vulnerable to the zero-day vulnerabilities.

Not all attackers are malicious or bad actors; some hackers are actually good and help companies protect their networks. In fact, there is an entire branch of InfoSec jobs known as penetration testers who are tasked with attempting to compromise a network’s security. Here you will review the various types of attackers and their function or role in network security.
### **Vulnerability Testers**
As the name suggests, a vulnerability tester is responsible for scanning servers and network devices for known vulnerabilities. There are a variety of vulnerability scanning tools on the market. Some are open source, such as Nessus, whereas most are commercial products.

Typically, vulnerability testers are the good guys, though hackers also play a similar role, which attempts to find new vulnerabilities in software that can be exploited.
### **Blue, Red, White, and Purple Teams**
If you really want to know how well your security can withstand an attack, add some friendly competition. The terms *red team* and *blue team* originated in the military. The red team attempts to compromise the security, while the blue team defends. There may also be a neutral white team that observes the festivities and may even serve as referee.

Some companies use this attack and defend approach but take it to the next level by involving continuous improvement in the process. This is referred to as purple team (remember that red and blue together make purple). In the purple team approach, the red and blue team engage, and then when certain success criteria are met, the teams debrief, cross-train each other, and repeat. This is also known as an iterate and improve model.
### **Hackers: White Hat, Black Hat, and Gray Hat**
If you have ever seen an old western movie or TV show, you may immediately recognize this reference to hat colors. In those shows, the bad guys typically wore black hats, while the good guys wore white hats. Fast forward to today: we now use the terms black hat, white hat, and even gray hat to describe InfoSec hackers and their intentions or ethical position.

White hat hackers are IT professionals who specialize in penetrating or compromising network security but only to help an organization improve its own security posture. Most importantly, they only perform the attacks when authorized to do so; to the fullest extent possible, they remain in compliance with any and all laws governing such behavior.

Black hat hackers, on the other hand, may or may not be IT professionals but possess the knowledge and will to breach systems for profit. That profit may be monetary, street credibility, or just a source of entertainment. Black hat hackers do not ask permission and are not interested in helping their targets improve their network security.

In the middle of the spectrum are gray hat hackers, a group of people who may or may not be IT professionals and may or may not choose to break laws in pursuit of their hacking goals. Unlike black hat hackers, gray hats have no malicious intent in their actions; unlike white hats, they may not have obtained permission to perform the attack.
### **Insider Threats**
Some of the most potent threats come from people within your organization. Because they have legitimate access to systems, they are in a position to hack from the inside of the network, often undetected. Furthermore, a disgruntled insider may have a motive. Whenever you combine motive and opportunity, you have a substantially increased risk of trouble.

But not all insider threats are malicious in nature. Accidents happen, and if your account was granted extraordinary privileges, you could accidentally impact a large number of users with a single incorrect command. In both cases, the key to success is restricting access to the minimum set of permissions needed to perform the job and enable audit logging of all administrative actions so that you can later determine the cause of mysterious changes.
### **Nation States**
Industrial espionage is not just a theme for movies, it happens in real life on the internet. In a global economy, knowledge is power. Knowledge takes on many forms but is usually based on data or designs that companies refer to as intellectual property.

Consider an example of a fictitious country called Atlantica, positioned in the middle of the North Atlantic Ocean. Atlantica has a large labor force but almost no high-tech industry. Companies from wealthier countries have been building factories in Atlantica to take advantage of the inexpensive labor but have refused to share their intellectual property with other native Atlantica companies. Atlantica develops a plan to steal the intellectual property by hacking these more advanced companies so it can acquire the knowledge and designs it needs to kick out the foreign companies and produce the products itself.

While the example is extreme and perhaps unlikely, intellectual property theft through industrial espionage is a very real concern for many companies. The bigger threat is that *nation states* have substantially larger budgets to hire hackers than the average criminal enterprise.
### **Script Kiddies**
Script kiddies are the copycat criminals of the hacking community. They typically hack out of pure curiosity or entertainment and often use poorly documented tools or scripts written by much more advanced hackers. Script kiddies have no formal InfoSec training and are typically not IT professionals. It is this lack of knowledge that contributes to the unpredictability of their actions and the random levels of damage that results from their attacks.

Also, contrary to the name, script kiddies are typically not kids at all; they may be adults who recently learned a thing or two about security and now wish to try things out in the real world, not realizing the consequences of their actions.

A threat is someone who or something that can exploit a vulnerability to attack an asset. That means that a threat could be a person (knowingly or not), a software program, or even a natural disaster.

Threats and attacks are as numerous as vulnerabilities and come in just as many forms. Consider some of the common network attacks below, including eavesdropping methods, discovery techniques, service disruptions, and remote code execution.
### **Wiretapping**
This term comes from a historical technique used to spy on telephone calls. At the time, telephones transmitted analog electrical signals over copper wires that ultimately led to the telephone company. If an eavesdropper knew which wires belonged to a particular home or office, they could use a special telephone handset equipped with wire clips, or "taps," to listen in on the call. The process of clipping the taps to the telephone wires became known colloquially as "wiretapping"; today, it refers to any process that allows an attacker to electronically eavesdrop on a conversation, whether between two humans or two computers.

This form of attack can also include putting special wiretaps in-line with a computer’s network cable and then using a device called a "packet sniffer" to listen and record the traffic on the network. Alternatively, the attacker may put the wiretap in the wall behind a network jack. Wiretaps often use a wireless transmitter to send its ill-gotten information to the attacker’s computer. In cases where the security team might notice the attacker disconnecting the target device to add the wiretap, an attacker could use a special listening tool that interprets the electromagnetic field (EMF) surrounding a network cable so that the attacker can eavesdrop on a wire without even breaking its connection. To combat this problem, some organizations exclusively use fiber optic cables in their high-security areas. Though fiber optic cables are not immune to all wiretapping attacks, they are immune to EMF listening devices because fiber optic cables use photons (light) instead of electrons to transmit information.
### **Port Scanning**
When planning an attack, the attacker needs to know what type of services or applications are running on the victim’s computer. Fortunately for the attacker, this is easy because most network-accessible services open TCP ports to accept connections from legitimate client computers. An attacker just needs to send traffic to each and every port to learn which services are running. Unfortunately for the attacker, there are tens of thousands of ports, numbered from 0 to 65,535.

It was not long before tools made this job easier. An application called a "port scanner" can systematically check each of these ports by sending thousands of TCP/IP packets to the victim’s computer, each packet on a different TCP port. The victim’s computer will discard requests made to ports that are not assigned to a running application or service. The port scanner is then able to see which ports respond and which do not, allowing the attacker to perform more in-depth scanning to determine what service is running and if there are any known vulnerabilities present in that service.
### **Taking Control**
Once the attacker knows which ports are accepting traffic, the attacker can run standard vulnerability scanners against the victim’s computer to learn if any of the services can be easily exploited. This is yet another reason why you should routinely use a vulnerability scanner against your own servers. Even if you do not, attackers will.

Database servers are a popular target of attacks because they typically contain high-value information. But, more importantly, vulnerabilities are often found in applications that allow users to query databases. A particularly well-known attack called a SQL injection allows an attacker to take control of a database server by inserting special commands into input boxes instead of entering basic text. For example, you may have an application that allows a user to search for products by keyword. The application has a form that reads the keyword and then adds it to a Structured Query Language (SQL) command that is executed by the database server. So, instead of entering regular text, the user or attacker enters a specially crafted string of text that includes SQL commands designed to take control of the server. In a perfect world, the developer of the application would include code that validates the text that the user typed and would discard it if anything other than text was found. Unfortunately, there are many applications that do not check the input carefully and end up running nefarious SQL commands that compromise the security of the database server.

Another popular attack vector is a buffer overflow. Much like the SQL injection attack, this attack is made possible by applications that do not properly validate user input for extraneous content. In this case, the attacker purposefully enters text that is too large to fit within a region of memory called a "buffer." By entering more characters into the buffer than it can handle, some of those characters will overwrite neighboring, potentially executable areas of memory. This is not a regular or even random string of characters, though; it is compiled code that contains executable instructions that will grant the attacker control of the server. Under normal circumstances, the CPU of the server would simply ignore the compiled instructions; however, if the attacker correctly guesses the exact size of the memory buffer, the attacker can overflow it and trick the CPU into executing the rogue instructions.

There are two main protections against these attacks. The first is to review your source code and verify that all user input fields are checked for robust or unplanned values. Second, enable the NX-bit (no-execute) functionality on the physical computer or VM to tag memory buffers as containing storage (data) or CPU instructions (code). With the NX-bit enabled, the CPU will only execute the contents of memory buffers identified as code.
### **Spoofing**
Hacking a server and taking control of it may get noticed quickly, so attackers may prefer the more subtle approach of eavesdropping to gain the information they desire. Unfortunately for the attacker, eavesdropping is becoming much more difficult due to advances in networking, such as the Ethernet switch. To gain access to the information, the attacker needs to get into the middle of the conversation; however, to do so, the attacker must impersonate the sender and receiver of the traffic. This act is known as "spoofing its identity."

Consider as an example a man-in-the-middle attack. An attacker wants to intercept the communication between a client computer (client) and a server. The attacker will likely use two network interfaces, one that is spoofed to look like the server and another to look like the client. When the real client attempts to contact the server, the attacker’s computer responds to the client and captures the request. The attacker then replays that connection request from their computer using the network interface that has been spoofed to appear as the client. The server exchanges information with the attacker believing the attacker is the client, and then the attacker forwards the response back to the actual client so that no one notices the break in connection. There are many variants of spoofing. Another example involves ARP poisoning, which is a method attackers use to cause an Ethernet switch to flood all traffic to every port on the switch, including the attacker’s computer.
### **Denial-of-Service (DOS)**
Threats are not restricted to data loss or gaining unauthorized access to data. There are also threats that make data or entire systems unavailable to anyone. Threats known as denial-of-service (DoS) attacks do just that: they deny someone access to a service, usually by overwhelming the victim with enormous amounts of useless traffic.

Many DoS attacks use features within the ICMP, such as "ping." You may recall that ping is a tool commonly used by network administrators to verify that a computer is online. When you ping a computer, your computer sends the remote computer an ICMP echo-request packet, which essentially asks the remote computer to reply back to you. When the remote computer receives the echo-request packet, it automatically responds back to the source address with an ICMP echo-reply packet. Herein lies the problem: attackers learned that they could forge the source address of the echo-request, making the target computer believe the request came from another location and correspondingly send its echo-reply packets to another computer rather than the attacker. This redirection allows the attacker to continue attacking with minimal stress on the attacker's computer.

This opened the door for a number of attacks, some of which no longer function due to fixes made within the TCP/IP network stack on modern computers and routers. The first was the ping of death, a trick whereby the attacker would send the victim a malformed ICMP packet that would cause the victim’s computer to crash or stop functioning on the network. After the ping of death was patched, a new attack called the "ping flood" took its place.

The ping flood attack overwhelms a victim’s computer with an immense volume of ICMP echo-request packets, all containing a forged, randomized source address. The victim’s computer automatically begins sending ICMP echo-reply packets to all these forged source addresses, which eventually overwhelms the victim’s computer so that it cannot do its normal job. Fortunately, even though the source address is forged, this type of attack typically can be stopped and sometimes even prevented. Its big brother, the "Smurf attack," however, is significantly more difficult to stop and prevent.

The Smurf attack is a distributed denial-of-service (DDoS) attack, which means that instead of one computer sending forged packets to a victim, in the Smurf attack potentially thousands of computers will bombard the victim. The difference is subtle but important. In the ping flood, the attacker sends an ICMP echo-request with a forged source address to the victim, forcing it to send a corresponding echo-reply packet to a random computer. In the Smurf attack, the attacker sends a forged ICMP echo-request packet to the broadcast address of a large IP subnet, which means that a massive number of computers would all receive the message. Instead of randomizing the source address as in the ping flood attack, in the Smurf attack, the attacker specifies the victim’s address as the source address. As each of the hundreds or thousands of computers receives the ICMP echo-request packet, each will respond by sending an ICMP echo-reply packet to the victim’s address, thereby crippling its network connection.

There are also other DDoS attacks that aim to exhaust a computer’s CPU or network connections, including SSL attacks, which cause the victim’s computer to consume excessive CPU time as it constantly sets up and tears down thousands of SSL encryption sessions over and over.
### **Social Engineering**
Trust is a weakness, at least in the InfoSec world. It is usually substantially easier to "hack a human" than hack a computer because people are generally trusting of others. This leads to a class of attack known as "social engineering," the art of manipulating human trust to gain access or information. Here are some examples.

- Impersonation:
  - You overlook or just do not report the person resembling an electrician who is working on a nearby wall outlet. (He is actually installing listening devices on your network cables.)
  - The helpdesk resets a user's password over the phone without being able to recognize the person's voice. (It was an imposter who can now log in as that user.)
  - You hold the door open for a man dressed in a courier's outfit who seems to be carrying very heavy boxes into the office. (The boxes contain his computer and additional hacking tools.)
- Phishing:
  - You answer phone or email surveys or sales calls that discuss the type or brand of your security devices or policies. (They were assessing your security posture and checking how difficult your defenses would be to overcome.)
  - You click links in emails from the IT department asking you to test a new web-based email that of course requires you to log in. (The email was fake and the web page is a phishing site that collects usernames and passwords.)
  - You accept that free USB drive from a new vendor and plug it into your computer. (The drive contains a stealth phishing tool that reports your keystrokes back to the hacker.)

One of the largest jobs in SecOps is managing the risk of attacks and acting both proactively to prevent or mitigate the damage and reactively to stop the attack and repair the damage.
### **Honeypot**
A honeypot is a server or device that is configured to look very authentic, potentially containing data that appears to be legitimate user data, or configuration files that seem authentic. It is also known as a "tar pit" because it is intended to attract or distract would-be attackers from the actual targets on the network. The goal of the honeypot is to provide a false positive for hackers, whereby the attacker breaches the honeypot and believes the data it contains is the target’s actual data.

An alternate use of the honeypot or tar pit servers is to collect data on the attacker. A tar pit server is similar to a honeypot in that it contains data that is fictitious; however, the tar pit server is also designed to slow down the attacker so that tracing information can be obtained by the intrusion detection system (IDS).
### **Proactive and Preventive Measures**
Think about the number of threats and attacks discussed in this course. Aside from powering off your computers, it is difficult to imagine a solution that could protect a network from all the possible attacks and attackers. Security is not a task that you check off on a list, but rather a constant process to be performed. It is like a perpetual game of cat and mouse.

To combat these problems you need to take an in-depth defense approach to network security. That means you will use multiple tools and methods together in an overlapping manner to create rings or layers of security. If one component is compromised, there should be at least one other obstacle to overcome before the attacker gains access to the system. Ideally, you will mix different methods of defense, such as using an application-aware firewall on the perimeter and then relying on an intrusion prevention system (IPS) within the network and firewall and anti-malware software on the servers.

Minimize your exposure by keeping your operating systems and applications up to date on patches. Additionally, follow security-hardening guides that help you remove unnecessary services and features that may be susceptible to attack.
### **Risk Mitigation Response**
If a virus is spreading throughout the network, you will not have time to develop a plan and then act on it. So, be sure to develop and test your containment plans before an attack occurs. The response plan should first help contain the damage from the attack, which may involve quarantining computers or severing network connections, and then work to remove the threat and clean up the damage.

Remember that some attacks are silent and may not cause obvious damage right away. Such is the case with rootkits, backdoor attacks, and Trojan horses. Run complete antivirus and anti-malware scans on your systems regularly and keep the virus definitions as up-to-date as possible to help you detect new and emerging threats.
### **The CIA Triad**
The confidentiality, integrity, and availability (CIA) triad is a reference model to help protect information from unauthorized disclosure and modification while ensuring it is accessible and intelligible to its authorized users. The CIA triad is composed of three principles: confidentiality, which limits access; integrity, which enables you to trust the information; and availability, which ensures that you have access to the information.

The CIA triad is a model that guides information security policies in organizations. We represent it as a triangle with points C for confidentiality, I for integrity, and A for availability.

Confidentiality refers to data privacy and sensitivity. You could classify internal records, like salaries, financial data, or product development documents as confidential and restrict access to this data.

Integrity is verification that the stored data you retrieve is untampered, unmodified, or unadjusted. For instance, a hacker wouldn't want to delete a log and raise a red flag. But what if the hacker edited the record to remove evidence of any alteration? In this situation, how do you prove data integrity?

Availability is about how accessible you want the information to be. Information on a public website is for wide distribution. Hence, it is entirely available and less private.

To determine where the security needs for a given data set fit in the CIA triad, picture a two-dimensional graph. The Y-axis is between confidentiality and integrity, and the X-axis is between confidentiality and availability. Your data will be somewhere in this triangle— from complete confidentiality to complete availability on the X-axis and between that balance of confidentiality and availability versus integrity on the Y-axis. You usually want to ensure that your data is not corrupted or maliciously modified. Of course, you do update documents and spreadsheets, but those don’t count as malicious modifications. Because you want a fair amount of integrity, your data might be at the top of the Y-axis near the “I.” Next, you need to decide where your data should be on the X-axis between confidentiality and availability. Something on a public website might be closer to A, while private information might be closer to C. You decide where your data fits within the C, I, and A points in the triad. You might mark information as sensitive, extremely sensitive, or top secret and accordingly decide where to store it, how to make it available, and who can access it. You might use checksums to verify data integrity. For this, you could use a file that lists all the files, each with a checksum, to make sure a hacker didn't maliciously delete data. You can control the availability of data with permissions. For this, you again decide how accessible you want to make it, where you store it, and who gets access to modify it.

To summarize, think about where your data fits within the CIA triad. Typically, your data would be more towards the upper end of the Y-axis and just about anywhere on the X-axis.
### **CIA Triad**
*Diagram: The CIA Triad*

Consistency is critical in security management; therefore, some organizations create specific classifications for their data that define different sensitivity levels, each with specific policies that explain how the information should be handled. These classification levels are then applied to every file, email, database, and even printed papers. When evaluating the sensitivity of a piece of information, all three of the CIA principles should be taken into consideration; however, depending on the type of information, you may focus more on one principle than another.
### **Confidentiality**
The confidentiality principle helps limit access to information, which, by definition, can contradict some of the recommendations found in the availability principle. The goal of confidentiality is to prevent an unauthorized user from accessing, copying, or transmitting the information.

Confidentiality is often equated to privacy because the two terms share many of the same characteristics, such as ensuring that only the intended recipient of the information can access it, following a need-to-know policy, and reducing exposure by destroying all copies of the information that are no longer needed.

There are many ways to breach or compromise the confidentiality of data if the proper precautions are not taken in advance and then routinely checked for accuracy and consistency.

- Unencrypted information is easy to steal and change.
- Deleted files are rarely purged from a disk immediately and often can be recovered with ease.
- The physical theft of a device gives an attacker an unlimited time window to break the encryption of your data.
- Social engineering is a method used by attackers to gain an unsuspecting victim’s trust to provide information, such as passwords or server names, or even just to gain physical building access.
- Accidents and malfunctions also play into the equation. For example, confidentiality of information can easily be breached by storing files in the wrong location, emailing data to the wrong person, or printing confidential information to a public printer.

Some methods that may help prevent compromises include:

- Where possible, encrypt the information at-rest (where it is stored) and in-transit (while it is moving across the network).
- Be sure to encrypt and physically secure your laptops, servers, portable hard drives, and even backup tapes/disks.
- Consider using a tool to securely delete files or overwrite them after deletion.
- Train your employees about social engineering attacks.
- Create and enforce a policy that ensures all users must use complex passwords (a combination of uppercase and lowercase letters, numbers, and symbols with a minimum length) and use multifactor authentication (MFA), such as biometrics or a digital key fob.
- Following the principle of least privilege (which means you only assign users the minimum permissions needed to perform their jobs), institute restrictive access controls on all data and provide access to information on a need-to-know basis only.
### **Integrity**
The integrity principle helps identify the trustworthiness of the information. It is possible to identify where the information came from and if the data has changed since it was originally sent. Integrity is a function that is often incorporated into encryption and, therefore, works well with the confidentiality principle.

Some of the compromises of data integrity include:

- Man-in-the-middle attacks, where an attacker changes the contents of the message after it was sent, but before it was received
- The intentional or unintentional deletion or modification of data
- Malfunctions in equipment that cause data corruption
- Natural phenomena such as electromagnetic pulse (EMP) attacks, which can destroy or severely corrupt data

Some methods that can be employed to help prevent the compromise of data integrity include:

- Require all data transmissions to use encryption or digital signatures to confirm the identity of the sender and to identify if the message has been changed.
- In cases where digital signatures will not work, use one-way hash calculations, such as SHA-3 (Secure Hash Algorithm 3), to create a value that can be used to verify the data has not changed.
- Use version control within your data storage to help you quickly revert accidental changes or deletions.
### **Availability**
The goal of the availability principle is to ensure that the data is always accessible by its authorized users. This includes aspects such as adding high availability to your server solutions and minimizing downtime by carefully managing your application updates and patches.

Some of the common actions that can compromise the availability of data include:

- denial-of-service (DoS) and distributed denial-of-service (DDoS) attacks, which prevent legitimate users from accessing the resource by sending an overwhelming amount of data to the target server;
- unplanned downtime due to server crashes or failed upgrades; and
- accidental changes to access control lists, which removes access for authorized users.

Some methods that can be employed to help prevent availability issues include:

- creating and maintaining a full disaster recovery plan that includes a full site failover as well as the method to restore data for individual servers;
- implementing server high availability where possible, employing clustering technology where appropriate; and
- setting up regular backups of your data and considering storing a backup copy at another physical location to protect against site-level disasters.

*Diagram. Firewall.*
### **Firewalls**
A network firewall is a barrier that intercepts and inspects traffic moving from one area of the network to another. Nearly all networks today benefit from the protection of some type of firewall. In all likelihood, you passed through several firewalls when you opened this material. You probably have a firewall at home that allows you to access the internet while simultaneously preventing intruders from accessing your home network. The servers hosting this material are also protected by a firewall that limits the type of internet traffic that can reach them.

Firewalls come in a variety of forms and provide a range of functionality. Some may be physical appliances mounted in data centers, while others may be virtual appliances operating as VMs (virtual machines). Still, there are others known as host-based firewalls that operate as applications running on workstations and servers. They all have something in common though; they have a set of rules that define whether the firewall will permit or deny the traffic to pass on to its intended destination. In the following sections you will explore firewall types, functionality, and terminology.

*Diagram. Packet filtering in the OSI model.*
### **Packet Filters**
A packet filter is a firewall that operates at Layers 3 and 4 of the OSI network model: network and transport.

In most networks today, that equates to the IP address (Layer 3) and the TCP or UDP port number (Layer 4) of the traffic passing through the firewall. These firewalls inspect incoming (ingress) and outgoing (egress) traffic and compare the following attributes to a database of packet filter rules that determine if the firewall will forward (allow) or drop (deny) the traffic:

- Protocol (typically IP)
- Source IP Address
- Destination IP Address
- Source TCP or UDP port number
- Destination TCP or UDP port number

These firewalls are only concerned with the address label (header) of the packets and perform no level of inspection on the contents of the packet (the payload). This means that potentially dangerous payloads could pass through a packet filter without being detected as long as the source and destination values were approved by the firewall rules.
### **Circuit-Level Gateways**
A circuit-level gateway is a device that operates as a middleman between two or more systems to help conceal the true identity of the client and server. The gateway may change the IP address and the TCP/UDP port number of the traffic to allow two networks to communicate that otherwise could not (for example, your home network and the internet).

Circuit-level gateways are the foundation of network address translation (NAT) and port address translation (PAT), which are commonly used in firewalls to allow private IP address ranges to communicate on the internet. These firewalls are only concerned with the address label (header) of the packets and perform no level of inspection on the contents of the packet (the payload). This means that potentially dangerous payloads could pass through a packet filter without being detected as long as the source and destination values were approved by the firewall rules.

*Diagram. A stateful firewall in the OSI model.*
### **Stateful Inspection**
To help you understand the significance of stateful inspection in firewalls, you must first understand the meaning of the term state. In this context, the word state refers to the connection state of a conversation between two computers. Some protocols, such as TCP, require that the recipient of a message respond back to the sender with an acknowledgment that it received the data. In a packet filter firewall, this would require at least two firewall rules: one that allows the sender to transmit data to the recipient, and another that allows the recipient to respond (acknowledge) back to the sender. Now consider the implications of a sender communicating with many recipients. There is still one rule for the sender, but now there are many rules for the acknowledgments because each recipient requires a rule to respond to the sender.

To reduce the number of firewall rules needed to support TCP communication, firewall vendors implemented a feature known as stateful inspection. This feature allows a firewall to identify traffic as conversational and automatically create temporary firewall rules to permit the response traffic to flow back to the sender. In this way, instead of maintaining a multitude of rules, in a firewall with stateful inspection, you only need to create a firewall rule that allows the communication to begin.
### **Application Level**
Remember, packet filter firewalls lack the ability to inspect the contents of the packets. Because of this, malicious traffic could pass into the network unchecked. To combat this potential weakness in security, network administrators began using proxy servers that could act as a middleman, reading and parsing the traffic payload, and then forwarding it on to the intended destination if the payload was safe. This behavior was later incorporated into firewalls to provide a deeper level of inspection. Firewalls with this ability are commonly called application-aware firewalls, or Layer-7 firewalls because application is the seventh layer of the OSI model.
### **Intrusion Detection and Prevention**
Intrusion detection systems (IDS) and intrusion prevention systems (IPS) are advanced security solutions that can identify malicious traffic based on a database of known behaviors and payload signatures. IDSs monitor the network to detect threats, whereas IPSs intercept and block threats.

Both types of systems can be configured to operate in tap mode, which is where they attach to the network as listening devices only. The name tap comes from the term wiretapping because these devices are essentially eavesdropping on the traffic flowing through the network. The tap mode works well for IDS devices because they are passive listeners on the network and are designed to alert a network administrator if they detect any suspicious behavior.

On the other hand, tap mode does not work as well for IPS devices, which are designed to intercept and block suspicious traffic. For an IPS device to stop traffic, it must be positioned in the middle of the traffic stream, a configuration known as in-line mode. Typically, IPS devices have many network ports that are designed to operate as input/output pairs. The network administrator will physically route cables through the IPS device to create choke points in the network. For example, you have a set of public web servers connected to a dedicated network switch that is connected to your firewall by one network cable. You acquire an IPS device so that you can scan the traffic entering and leaving that network. To do so, you will disconnect the existing cable that connects the switch to the firewall and then place the IPS device in-line by connecting the firewall to one of the ports in an input/output pair and the switch to the other port in the pair. The IPS device bridges the traffic and appears invisible on the network, yet it is actually inspecting every packet and copying it from one cable to the other.

If the IPS device detects an anomaly, such as an unknown packet type or pattern of traffic, it can alert the administrator or block the traffic immediately. If it detects malicious traffic, it typically blocks the traffic automatically. There are also cases where the IPS device may block traffic, particularly files that are known to carry viruses and malware. This is a method known as reputation-based protection, because the file is blocked based on how often that type of file is found to carry viruses or malware. A good example of this is when IPS devices and firewalls block executable (EXE) attachments or downloads.

Like firewalls, IPS and IDS devices are not always physical appliances. They are also available as virtual appliances and as host-based IPS/IDS applications, which can be installed on your servers or workstations.

*Diagram. Intrusion detection system (IDS).*
### **The OSI Model**
Think back to your early lessons on networking and you will likely remember the Open Systems Interconnection model (OSI model) from Unit 2, a conceptual framework that has defined the way computers communicate over networks since the early 1980s. You may recall that one of the motivations for creating the OSI model was to reduce the proprietary nature of networking and increase innovation. Prior to the publication of the OSI model, the computer system manufacturers, such as IBM and Digital, were the primary providers of all network adapters, devices, and protocols used to interconnect their computer systems.

*Diagram. The seven layers of the OSI Model*

Today, computer systems are able to intermingle products, services, and protocols from different hardware and software vendors thanks in part to the well-defined layers of the OSI model.

Each layer represents a part of the computer-to-computer communication process and includes definitions for one or more protocols, services, and devices that satisfy the requirements of that part of the communication process. For example, Layer 2 (data link) includes protocols, such as 802.3 Ethernet and 802.11 Wi-Fi, each of which defines how computers can share access to a common medium such as a wired or wireless network.

The OSI model is often used for troubleshooting network connectivity, but it is equally important to network security assessors when analyzing the network for security threats and vulnerabilities. The layers of the OSI model neatly organize the various protocols, services, and devices in use on the network, and over time the OSI model has enabled developers to create new protocols and services that operate at specific layers. These new protocols and services have, in turn, given rise to new security threats. By using the OSI model as your guide, you can discover how your devices connect together, identify the protocols and services that they use, and then scan them for vulnerabilities and threats that put your network at risk.
### **Layer 1: Physical**
The Physical layer represents the physical medium that connects the computers together. Each medium will have different strengths and weaknesses. For example, consider the security-related differences between a wired network using Category 6 cables versus a wired network using fiber optic cables. If wiretapping or electronic eavesdropping is a concern, the fiber optic cables provide greater security.

Wiretapping is a Layer 1 threat because it involves tampering with the physical cables of a victim’s network. Copper-based wiring, such as Category 6 cabling, is susceptible to electronic sniffing or listening devices because the electrons flowing through the cables create a perceptible electromagnetic field (EMF). Fiber optic cables use light waves instead of electrons and therefore do not emit an EMF signature that can be captured and interpreted.

In your security analysis, you should categorize all physical vulnerabilities and threats as Layer 1 risks. For instance, check the security of the locks on the doors to the data center, equipment racks, and wiring closets throughout your building. Though most of the threats and vulnerabilities at Layer 1 do not seem to directly relate to traditional network security, be careful not to neglect Layer 1 (physical) security. If an attacker can gain physical access to a device, your countermeasures at higher levels of the OSI model may be significantly impaired.
### **Layer 2: Data Link**
As mentioned earlier, where Layer 1 represents how a computer physically connects to the network, Layer 2 represents how computers logically connect to the network. Protocols at Layer 2 define how computers can share access to a common medium, such as a wired or wireless network. This includes protocols such as 802.3 Ethernet and 802.11 Wi-Fi.

Sometimes it can be difficult to determine at which layer of the OSI model an attack occurs. For instance, there is a very fine line between wireless attacks that occur at Layer 1 versus attacks that occur at Layer 2. Consider the following two attacks with very similar results.

An attacker executes a radio jammer attack at Layer 1. The radio jammer sends radio signals, which interfere with the victim's wireless network card and prevents the victim from communicating with a wireless access point (WAP).

Alternatively, the attacker may execute a wireless deauthentication attack at Layer 2 that sends a special wireless frame to a WAP that disconnects the victim's computer from the wireless network.

Both are denial-of-service (DoS) attacks, but the radio jammer attack will affect a large number of users simultaneously, whereas the deauthentication attack is a targeted attack. Additionally, attackers can use the deauthentication attack to capture and then later decrypt the wireless network password sent between the victim's computer and the WAP. While you may not be able to stop the DoS impacts of these attacks, you can minimize your risk of a deauthentication attack revealing your wireless password. To do so, use WPA2 or WPA3 wireless security with a very long and complex passphrase. This does not eliminate the risk, but the long and complex passphrase will resist brute force password crackers longer than a short, simple passphrase.

Wired networks are just as susceptible to attacks at Layer 2. Consider the Ethernet-based attack known as ARP poisoning, which allows an attacker to eavesdrop on all network traffic sent through an Ethernet switch. The attacker sends special Ethernet frames on the network that quickly poison or overwhelm the switch's internal traffic-handling database called the forwarding information base (FIB). Without the FIB, the switch no longer knows where to forward traffic, and so it begins flooding all its ports with every frame it receives, allowing the attacker to receive a copy of all the traffic passing through the switch. When coupled with a packet sniffer, the attacker can reconstruct and analyze the received frames to gather information that will help the attacker to conduct further attacks against the network. An intrusion prevention system (IPS) is one of the best defenses against these attacks.

Another Layer 2 attack targets Ethernet switches that separate traffic into multiple logical networks called virtual local area networks (VLAN). When configuring a switch with VLANs, the network administrator typically configures each switch port to operate in one of two modes: access mode (also called untagged mode) or trunk mode (also called tagging mode). Access mode ports are used by servers and workstations and are assigned to a single VLAN. Trunk mode ports are used for the ports which interconnect multiple switches or routers and carry the traffic of multiple VLANs.

When a computer is attached to an access port, it may only send and receive traffic on the VLAN assigned by the switch; the computer has no control over the VLAN selection. On the other hand, if a network administrator mistakenly configures a computer's switch port as trunk mode, the computer may be able to send and receive traffic on any VLAN on the switch. If an attacker gains access to a computer connected to a trunk port, the attacker can perform a VLAN hopping attack to join a network that would normally not be available to them. The good news is that this attack can be mitigated by following the switch vendor's recommendations for VLAN configuration, as well as by designating all computer switch ports as access mode instead of trunk mode.
### **Layer 3: Network**
The Network layer is so named because it allows computers on different networks to exchange data. This is where you will find the IP and ICMP protocols, which belong to the TCP/IP protocol suite. You might remember the ICMP protocol from earlier lessons because it contains a number of functions that test the connectivity of network devices. For example, the echo-request and echo-reply combination commonly referred to as ping.

Ping attacks, such as the ping flood DoS attack described in an earlier lesson, are typically intended to disrupt communication on the network. However, an attacker can also use ping for information gathering. For example, the ping sweep attack sends pings to a large number of IP addresses to detect which computers are online and may, therefore, be susceptible to other attacks. Fortunately, these attacks can be easily mitigated by using a packet-filtering firewall.

Spoofing is an attack that can occur at both Layer 2 and Layer 3. The goal of a spoofing attack is to impersonate another computer's IP or its Layer 2 media access control (MAC) address. The attacker configures a network card on their computer to impersonate the victim's computer, then sends a special frame onto the network that directs Ethernet switches to forward the victim's traffic, specifically traffic that was originally destined for the victim's IP or MAC address. Intrusion prevention systems (IPS) are best able to prevent this type of attack.
### **Layer 4: Transport**
The two most common protocols within Layer 4 are the TCP and UDP protocols, both of which belong to the TCP/IP suite. Where Layer 3 protocols such as IP use a 32-bit number (for IP version 4) or a 128-bit number (for IP version 6) to uniquely identify each host on a network, protocols at Layer 4 further refine this addressing by using a 16-bit number, called a port number, that uniquely identifies each service on a particular host.

The Transport layer ensures that data received from the upper OSI model layers are delivered according to the needs of the application, for instance, managing a connection-based or connectionless session between the hosts. The applications do not need to be aware of how the transport layer maintains its connectivity.

TCP and UDP play an important role in the way applications communicate. TCP is a connection-oriented protocol, whereas UDP is connectionless. A connection-oriented protocol, such as TCP, provides delivery confirmation for data sent between two computers. If the sender does not receive confirmation of the delivery, it will retransmit the data to ensure it is eventually received. Consider the example of sending an important letter to someone by postal mail. If you want to ensure the recipient gets the message, you can pay for delivery confirmation services. When the letter is delivered, the postmaster sends you a comment card in the mail confirming its receipt. This is similar to TCP traffic where a confirmation is sent back to the sender. UDP works more like standard mail. You place the letter in your outgoing mail without any confirmation that it will arrive, and the receiver is under no obligation to alert you if they get the message.

When a service is run on a computer, that service will open specific ports at the transport layer to allow the service to receive incoming connections from other computers. For example, the DNS service opens TCP port 53 to allow computers to perform name-to-address resolution. An attacker wishing to know which services are running on a victim's computer could run a tool called a port scanner that, as the name implies, allows the attacker to scan the victim's computer for open ports that they could later attack. A packet-filtering firewall is an appropriate defense against port scanners.

In some cases, network administrators redirect a commonly known port number such as TCP port 80, which runs HTTP, to a less expected port number such as TCP port 8080, in hopes of obscuring or hiding the fact that the computer is running a web server. This practice is known as port redirection, and while it may fool a novice attacker, simply changing the port number of a service will not fool a more advanced attacker. In this case, obscurity is not security.
### **Layer 5: Session**
The Session layer allows computers to differentiate between the connections within a service on the same host. This would be similar to you keeping track of different conversations that you are having with the same person. You might talk about one topic for a while, then switch topics back and forth as you chat. In networking, remote procedure call (RPC) is an example protocol at Layer 5 and is used by computers to execute functions and procedures on other computers, such as a central server launching a program or print job. RPC has often been the target of many attacks over the years, but with regular operating system and application patching, you can mitigate most of these attacks.
### **Layer 6: Presentation**
The Presentation layer serves as a translation and security layer between applications, allowing computers to encode and encrypt data. Encoding is the process of writing data in a particular manner, such as a standardized file format like XML or GIF. Encryption is the process of concealing data, and despite its name, is commonly performed at the Presentation layer using Transport Layer Security (TLS), the replacement for the now deprecated SSL protocol commonly used to secure web pages.

Both SSL and TLS have been subject to a number of attacks that target weaknesses in the protocol itself. The goal of many of these attacks is to create a man-in-the-middle attack where the attacker inserts themselves between the victim and an encrypted target, such as the website of a bank. The attacker then impersonates the bank's web security by exploiting a vulnerability in the encryption or by fooling the victim into accepting a false security certificate. The attacker completes the attack by encrypting the traffic between themselves and the victim and then encrypting the traffic between themselves and the target (the bank in this case). The attack is named after the fact that the attacker becomes the man-in-the-middle of the conversation, able to see everything the victim does without encryption. Many of these attacks can be mitigated using an Application-layer proxy or an IPS, though training the users about fake security certificates is equally important.
### **Layer 7: Application**
The Application layer defines how users connect with the application services through protocols such as HTTP. It is the final layer of the OSI model, and like the first layer, it is host to as many different attacks as there are applications. An example of the Application layer is an application programming interface (API) endpoint for web services and websites, both of which leverage the HTTP and HTTPS protocols. Prime targets are web servers, especially web servers that host APIs.

For example, consider that you have created a mobile application to allow your customers to update their information in the customer database located within your corporate data center. The database is not directly accessible on the internet, so you created an application server to perform the read and write requests on behalf of the mobile application. Finally, you created an API to expose a subset of the application server's functions to the mobile application over the internet via the HTTPS protocol.

When a customer uses the mobile application to update their information, the mobile application interacts with the application server via the API. The API calls a specific program or function within the application server to complete the mobile application's request. For security, you configured the mobile application to send the customer's company name along with any requested updates to the API server so that the application server only updates that customer's data when performing database read/write operations.

Analyze this a bit. Though you did use HTTPS to encrypt the data between the mobile application and the API, the application is not yet secure. You incorrectly assumed that the API would only receive requests from your mobile application, and therefore the API did not require any special login or authentication process before it forwards requests to the application server. Unfortunately, this means anyone with a web browser can connect to your API and interact with your application server and data. The HTTPS commands that invoke the API calls are human readable and well documented. It is only a matter of time before an attacker discovers the commands needed through trial and error and then gains access to your data. Now that you know the vulnerability you can probably guess the mitigation. You need to implement authentication between the mobile application and the API.

Even after securing access to the front door of an application or API, you must ensure the application is free of known vulnerabilities. Security professionals use a tool called a vulnerability scanner to detect problems and known bad code that result in vulnerabilities in your applications. The list of vulnerabilities is large and ever increasing. These scanners can help you identify weaknesses that could lead to SQL injection attacks, buffer overrun attacks, and a variety of others that may allow an attacker to take control of your server.

Another way to mitigate attacks at the Application layer is to leverage a reverse proxy system that is able to scan the incoming packet for malicious behavior instead of simply forwarding the malicious payload to the destination. An IPS device will also protect against these threats.

In cryptography terms, unencrypted data is referred to as being sent or stored in the clear, meaning that the data can be read by anyone who intercepts the communication or accesses the unencrypted file. Encrypted data is referred to as ciphertext because the proper term for an encryption algorithm is a cipher. These ciphers come in many forms and rely on different types of keys to encrypt and decrypt the data.

If the cipher uses the same key to encrypt the data as it does to decrypt the data, the cipher is said to use a symmetric key. Conversely, if the keys are different, the cipher is said to use an asymmetric key pair, one key to encrypt the data and another to decrypt it.

Encryption uses several kinds of algorithms. Two commonly used methods are symmetric encryption and asymmetric encryption.

Symmetric encryption uses a key to encrypt and decrypt data. Think of the front door to your house. You use one key to lock and unlock the door; the same key does both tasks; this is similar to how a symmetric key works. An example of symmetric encryption is the Advanced Encryption Standard or AES. AES has several versions, such as AES-128 and AES-256. AES-256 is traditionally used today in most enterprise-type applications, such as banking or credit cards. AES is a symmetric key that allows encrypting and decrypting data with the same key.

Asymmetric encryption, on the other hand, uses two different keys to access data. An example is a Secure Sockets Layer or SSL. In the internet Hypertext Transfer Protocol Secure or https, the S stands for Secure Sockets Layer. Here the sender and the user use two different keys to access data. However, because SSL has some inherent vulnerability that hackers tried to exploit, Transport Layer Security or TLS has replaced SSL. Although it’s still called https, technically, no one uses SSL. TLS comes in multiple versions—1.0, 1.1, 1.2, and 1.3. The incremental numbering shows the latest version. Another example of asymmetric encryption is Internet Protocol Security or IPsec; it is often used for encryption of traffic in-flight on an existing network. The two keys or key pairs used in asymmetric encryption involve a public key and a private key. Together they are used to set the encryption. The public at large holds the public key in traditional encryption. An example is VeriSign. When you go to a website that has an associated secure sockets layer— TLS—certificate designed just for encryption, its certificate will be published by a well-known provider. Your browser will look, recognize VeriSign, and quickly check to see if the certificate is still valid. If so, the server will have a private key that is known only to that server. The server will then use its private key to go back and sign the communication.

Symmetric encryption often works in tandem with asymmetric encryption. For example, after the initial key exchange, AES runs behind the scenes to encrypt the data back and forth. This process is a common way of doing encryption with a public-private key pair. In addition to the previous encryption examples, key pairs are found in things like digital signatures. For instance, DocuSign uses a public and private key pair, with the public key held by the other organizations that want to verify the signature and the private key held by the person signing the key, or the actual document. We also use public-private key pairs to log into systems such as Linux.

To summarize, symmetric and asymmetric algorithms are standard methods of encryption. Symmetric encryption is simply one key, while asymmetric encryption uses public-private key pairs. Both types of encryption are often used together.
### **Symmetric Key Encryption**
Symmetric key encryption, also known as private key encryption, uses the same key to encrypt the data as it does to decrypt the data, meaning that when used for data transmissions, symmetric key encryption requires that both the sender and the receiver possess the same cipher key.

Encryption is ciphering data before transmitting it to the intended recipients to prevent interceptors from reading it. The intended recipients have a decryption key to decipher the data. Two commonly used methods of encrypting data are symmetric encryption and asymmetric encryption.

In symmetric encryption, a key management system, or KMS, also known by other names, generates a key. This key combines with the data, such as plain text, Word documents, spreadsheets, and audio or video files, to encrypt it. The encrypted data is then stored in a system along with the key. This would seem like keeping your house key in the deadbolt lock—convenient but insecure. However, the KMS generates a master key to encrypt the key that is stored with the data. The master key does not leave the KMS to prevent it from being corrupted, stolen, or viewed during transmission. With the key that is stored with the data being encrypted, anyone breaking into the storage will not be able to decrypt the data. To be able to retrieve the data into usable form, you need to get the encrypted data and the encrypted key out of the storage and into the KMS. The KMS will then use the master key to decrypt the data, so that you have the symmetric key and the encrypted data. You can then use the key to decrypt the data into its original form.

The final consideration in this process is protecting the master key, also called the key-encryption key, from being lost or compromised. The solution is to encrypt the master key with another one, and then encrypt that one with a third through the key encryption hierarchy. While it is possible to have numerous keys, the number generally does not go beyond more than three or four keys because it is necessary to break one key to get to the next. The system rotates these keys periodically, meaning the odds of any of them getting compromised are small. To summarize, symmetric encryption is a means of securing data. It requires a key management system, master key, symmetric key, and storage.

The security of the encryption depends heavily on the complexity of the cipher and the key, but this is usually not the weakest link. The sender and receiver must somehow exchange the secret key with each other before encrypting any data. It is during this key exchange that the encryption is most vulnerable. If an attacker were to intercept the key, the attacker could decrypt the data and even send a forged message by re-encrypting the data using that same key. You may have used symmetric key encryption in the past. Consider a group of classmates plotting to exchange secret notes in class. They first agree upon a cipher, such as changing each letter to another letter. Then they agree upon a key, such as the number of letters before or after the original letter. With that information, you could both encrypt and decrypt the messages. Unfortunately for these classmates, not only is the cipher simple to determine, but the key is easy to guess, having only 26 possible combinations.

*Diagram. Symmetric key encryption.*

This does not mean that symmetric encryption is weak. A sufficiently complex cipher and key can keep data secret for a long time. For example, in World War II the German army used a symmetric key cipher in their Enigma machine, an advanced cryptograph that incorporated the date into the key, rendering the code unbreakable by Allied forces for years.

In addition to its simplicity, the other advantage to symmetric key ciphers is the speed at which they can calculate the encryption and decryption sequences. When encrypting a large amount of data, it is significantly faster to encrypt the data using a symmetric key cipher than an asymmetric key cipher, but as with the Enigma machine, if given enough time and computing power, the key will be eventually compromised, and the code broken.
### **Asymmetric Key Encryption**
Asymmetric key ciphers rely on two different keys to encrypt and decrypt the traffic. This is particularly useful on the internet where the encryption of the data being sent to and from e-commerce and banking websites is needed. The customers have no reliable and secure means to exchange a secret key prior to the data transfer, so public key infrastructure (PKI) is relied upon.

PKI is an asymmetric key solution that allows two parties to exchange encrypted data without having first exchanged a private or shared key with one another. In PKI systems, each party that could either send or receive encrypted data must first create a key pair consisting of a public key and a private key. The key pair is created using an algorithm that enables one key to decrypt ciphertext that the other key has encrypted. Once the pair is created, the public key is published to a public repository, whereas the private key is kept secret by the owner of the key. If you wish to send this person an encrypted file, you would retrieve their public key from the internet and then use it to encrypt the file. You could then send the encrypted file to the person or even post it for them to download. The only way to decrypt the file is to use the recipient’s private key, which should be stored in a very safe place.

In addition to encrypting data for secrecy, PKI can also be used for nonrepudiation and to verify the validity and integrity of data that was sent. If you encrypt a file using your own private key, the data can be decrypted by anyone who can download your public key. This is not a security breach, it is a feature known as a digital signature. By encrypting the data, you create a ciphertext block that cannot be decrypted if it is altered after you signed it, thereby guaranteeing that the data has not changed since you signed it.

One downside to asymmetric key ciphers is the computational power required to perform the encryption on large blocks of data. This has led to a hybrid use of symmetric and asymmetric ciphers in bulk data encryption as well as for data encrypted in transit. Transport Layer Security (TLS) encryption, as well as bulk data encryption, is performed using a symmetric key to optimize its speed, but that key is exchanged using an asymmetric key cipher to ensure perfect secrecy of the key exchange. In this way, you get the best of both worlds: speed and security.

*Diagram. Asymmetric key encryption.*
### **Elliptic Curve Cryptography**
*Diagram. Example of elliptic curve cryptography.*

One of the weaknesses of PKI is that it is based on mathematical formulas. With modern computers able to process incredibly complex equations, the secrecy of these keys became very uncertain. In order to add complexity to the keys that defies current brute force attack methods, a new breed of asymmetric key creation was unveiled: elliptic curve cryptography (ECC). ECC uses the algebraic structure of elliptic curves to create a key that is even smaller than traditional asymmetric keys, yet it is substantially more difficult to crack without the aid of quantum computers.

You encrypt and decrypt data every day of your life. It could be when connecting to banking or healthcare websites or when you connect to your corporate network via a virtual private network (VPN) tunnel, as you will learn more about in this lesson. Encryption comes in a variety of forms and is used to protect data flowing through a network or when the data is stored on a disk. Data moving through a network is known as data in transit, while data that resides on a disk is known as data at rest. When the data is both encrypted in transit and at rest, you have end-to-end encryption. End-to-end encryption means that the data is never stored or transmitted in the clear.
### **SSL/TLS Encryption**
*Diagram. How SSL/TLS encryption works.*

Secure Sockets Layer (SSL) and Transport Layer Security (TLS) are the most common forms of encryption found on the internet today. Though SSL was deprecated in 2015, TLS provides similar functionality with more robust security. TLS creates a secure channel over the internet between a client computer and a server by exchanging a public key in the form of a certificate. The certificate is issued by a registered and well-known certificate authority (CA), such as Verisign. The certificate has a public key that is stored on the webserver and presented to the public whenever a user connects to the website. The private key is also stored on the webserver but is kept secret and protected.

When a client computer (the web browser) contacts the webserver, the client initiates an encryption handshake that establishes a symmetric key that will be used to encrypt their traffic. The key exchange starts with the client computer encrypting a token with the webserver’s public key. This ensures that only a computer holding the private key, the webserver, can decrypt the token. The webserver then decrypts the token using its private key, and then uses the token as a private key in the symmetric key cipher with the client, thereby ensuring both sides of the communication and the key exchange remain encrypted.
### **IPSec Encryption**
*Diagram. How IPSec encryption works.*Diagram Description

Internet protocol security (IPsec) provides an authentication and encryption solution that secures IP network traffic at Layer 3 of the OSI model. This is in contrast to the TLS protocol discussed above, which operates at Layer 6. TLS also differs in that it provides a PKI encryption method commonly used to encrypt web pages or data sent between one client and one server. IPsec, on the other hand, is commonly used to create virtual private network (VPN) tunnels across the internet or other untrusted networks to allow many computers to communicate with each other.

Though it is possible to have a client-to-server IPsec VPN tunnel, they are more often found in site-to-site configurations between devices, such as firewalls or routers. By terminating the encryption on a network device instead of a client computer or server, the computers are freed from the burden of encrypting and decrypting their traffic when communicating across the tunnel.

IPsec traffic is encapsulated and authenticated, which allows the devices to create an encrypted tunnel that traffic may pass through. This encapsulation hides the fact that the packets are flowing across an untrusted network, such as the internet, and gives the client computers the illusion that they are directly connected to one another or at least within the same network.

IPsec is composed of the Authentication Header (AH) protocol, which provides data integrity for the connection, the encapsulating security payload (ESP), which provides encryption for the connection, and the security associations (SA), which define the algorithms to be used and the key exchange method. IPsec is often used with internet key exchange (IKE) and IKEv2, though it is also possible to use IPsec with symmetric keys (more often called pre-shared keys because both devices must be configured to use the same key during the VPN negotiation process).
### **Common Data-at-Risk Algorithms**
When data is stored in a permanent or semi-permanent state, the data is said to be at rest. Data at rest should be treated with the same level of care as data in transit because servers and their components can be physically stolen or compromised in such a way that the data is left in the clear and exposed to attackers.
### **Advanced Encryption Standard**
You may have guessed from the earlier lessons that for bulk encryption of data, the preferred choice of ciphers would be a symmetric key algorithm. Currently, the most secure algorithm for storing and encrypting data at rest is the Advanced Encryption Standard (AES), a symmetric key cipher that makes use of different key and block sizes and creates a near-impenetrable encryption. AES modifies the plaintext input by performing a series of transformations on the data to create ciphertext. Depending on the key length, as many as 14 transformations can be made against a given block of data, making it exceptionally difficult, if not impossible, to reverse the encryption without the key or the aid of quantum computers.
### **Data Security**
Data classification, access control, data protection, and encryption all play a role in protecting data and collectively affect an organization's decision to adopt a particular cloud deployment model. Some cloud providers' backend architecture is more isolated from the public internet, while others leverage the already widespread access to the public internet. However, relying on the public internet may introduce regulatory or security concerns. This is just one of many aspects that should be considered when adopting any cloud deployment for an organization.
### **Data Classification**
*Diagram. Hierarchy of data classification.*

Whether you are storing data in a private or public cloud, it is equally important to identify the types of data that you are storing and to create policies that describe how to handle the data. These policies may include the length of time the data is permitted to exist, its security sensitivity, and any requirements that dictate the physical or geographic location where the data must reside.

Retention policies dictate how long a piece of data should remain available, whether in active day-to-day storage or in archive copies. Some types of data, such as financial transactions or security audit logs, must be kept for extended periods of time, although they may not be viewed often (or ever). Storing data for a shorter (or longer) time than permitted can result in serious fines. Many public cloud providers offer storage solutions that include data retention policies that can protect data from accidental erasure and then automatically delete it when the retention period lapses.

Your company may deem some data too sensitive to be stored within a public cloud provider. Therefore, you will need to set aside a location within your private cloud or corporate data center to house such data. In other cases, your data may be subject to data residency requirements that stipulate the physical or geographical location where the data must reside. Data residency requirements are occasionally seen in government organizations where public records may not leave the state or territory where the information was obtained.
### **Data Protection**
A common misconception is that data stored in the public cloud is automatically backed up and can be restored at any time. The reality is that your data is typically not automatically backed up by the cloud provider. That can also be true of private clouds. Always check with the provider to determine if a backup of your data exists, and if so, how often it is created, how long it is kept, and how you can retrieve it. If a cloud provider does automatically back up your data, be sure to include the length of time the backup copy is available when determining your data retention policies.

When working in a hybrid environment, you may consider backing up your public cloud data to your corporate data center. However, be warned that this could be a very expensive decision. Though most public cloud providers do not charge for data transfers into their cloud, most providers charge network transfer fees for all data leaving their cloud. Data backups from the cloud to your on-premises data center could include a very large amount of data each night. On the other hand, you may opt to take advantage of the free inbound traffic (if offered) to back up your private cloud to the public cloud.

Regardless of your backup solution plan, be sure to test it regularly. In fact, consider automating a restore process once a week to check for failures or inconsistencies in the backup.
### **Encryption of Data at Rest**
You should strongly consider encrypting all your data at rest, even if you own the server or storage hardware, as is often the case with private clouds. This is especially true for data in public clouds where you do not directly control the physical access to the servers that contain your data. Data encryption at rest provides a physical safeguard for your data because even if the server is stolen and physically removed from the datacenter, the data remains protected and inaccessible to the attacker.

Remember that encryption is only as good as the strength and security of the encryption keys. When encrypting data at rest, you must also consider how you will manage the encryption keys that are used to encrypt and decrypt your data. The key used to encrypt and decrypt your data is called a data encryption key (DEK). If an attacker obtains the DEK, the attacker can decrypt and access your data. Worse yet, the attacker may be able to decrypt the data, modify it, and then re-encrypt it without your knowledge.

Since an attacker obtaining the DEK is the vulnerability, the mitigation must be to prevent the attacker from doing so. You can do so in one of two ways. You could rotate or change the DEK regularly so that even if the DEK were compromised the attacker would have a limited time where the key was useful. Alternatively, you could seek a method that does not require you to disclose the DEK to anyone, and yet still encrypt and decrypt the data using that DEK. While the second option sounds impossible, it is actually the best choice. Changing the DEK used to encrypt data is a time intensive process because it requires you to first decrypt the data using the original DEK, then encrypt the data using the new DEK. If you have more than a few terabytes of data, this will take a very long time and incur a significant amount of CPU processing time to complete each time you change the keys.

That leaves you with the seemingly impossible option: never disclose the DEK. But how can you encrypt or decrypt data if no one has access to the DEK? The answer is simple, you will protect the DEK in the same way that you protect your other data files, encrypt it. When you encrypt the DEK, you use an asymmetric encryption key called a key encryption key (KEK). To help keep all these keys organized, you will store the encrypted DEK inside a key management server (KMS) that grants access to the key based on the validity of your KEK.

That is a bit complex, so consider the following analogy. Imagine that you have a secret decoder ring that you wish to lock up and protect within a vault. The secret decoder ring is your DEK and is used whenever you have a piece of data that must be encrypted or decrypted. The vault is the KMS, and the key to the vault is your personal KEK. Periodically you will change the locks on the vault to safeguard against KEK theft. Fortunately, that will not affect your secret decoder ring (the DEK). The DEK is still used to encrypt all your data, but the DEK is only ever disclosed in its encrypted form. When it is time to encrypt or decrypt data, your computer uses the KEK to temporarily unlock the DEK, which is then passed to the storage system to enable the encryption or decryption process.

Many public cloud providers offer a KMS for your encryption keys. This is also referred to as managed key encryption because your encryption keys are managed by the KMS.

Encryption of data at rest can also safeguard against accidents such as exposing data after a server has been decommissioned. For example, consider a server that is no longer needed and is then removed from the network without first performing a full security wipe of its hard drive. If the data on that hard drive was not encrypted, it may still be readable and end up in the wrong hands when the server is later sold or repurposed.
### **Encryption of Data in Transit**
It is equally important to encrypt your data in transit, also known as data in flight. Again, while beneficial in a private cloud environment, it is particularly important for public and hybrid clouds due to the potential for accidental data exposure on shared network links. Even in private clouds, the prevalence of malware makes data encryption important to protect against data theft or the manipulation of data in transit. In the case of a hybrid cloud, the wide area network (WAN) link, which connects the private and public clouds, should also be encrypted.
### **Application Security**
If you are developing a web application that will require internet users to create credentials for your application, consider instead using an authentication service known as federated identity management. Federated identity management allows internet users to authenticate to your application using federated identity servers at Google, Facebook, Twitter, and other sites where they may already have an account.

To clarify federated identity management, consider the example of a user named Bob, who wishes to log on to your web application. Bob visits your web application and notices that he can log on to your application using his Google ID. When Bob clicks the Google authentication button, his browser is temporarily redirected to Google, where he is prompted to log in using his Google credentials. After Google confirms Bob’s identity through a correct password exchange, the Google federated identity services send to your server a special token that uniquely identifies Bob, but does not disclose any of Bob’s private information. If Bob logs on again, Google will send you the same token value. This means that, instead of requiring Bob to create a unique username and password within your application, your application can rely on the value of the federated identity token to uniquely identify Bob when he logs on again in the future. Federated identity management relieves you from the security risk and liability of hosting a database of usernames and passwords, and it frees Bob from the burden of memorizing yet another username and password.
### **Access Control**
Once you have classified your data, you should determine who requires access to the data and to what degree they should have access. For instance, you may have data that everyone, even anonymous internet users, should be able to read and access but not change. Regardless of public or private cloud, always assign users the least amount of access required, and whenever possible, assign the permissions to groups instead of individual user accounts.

In private clouds, you can assign permissions to data using internal security authorization controls, such as Windows or Linux file server permissions assigned to users or groups found in your Microsoft Active Directory listing. In public clouds, you may or may not have the ability to assign users and groups to the cloud storage locations. This is dependent upon the cloud provider and the way you have integrated your authentication and authorization systems with the cloud provider's identity and access management (IAM) services.

When working in hybrid cloud environments, you should first configure the public and private cloud providers to use the same IAM configuration. This standardizes the user and group names across the clouds before assigning permissions to the data, which in turn helps avoid accidentally assigning extra (or insufficient) permissions due to user or group naming similarities and inconsistencies.
### **Network Security**
Publicly accessible servers on the internet are constantly exposed to attack and, when compromised, can allow an attacker to launch additional attacks inside your network from the comfort of the attacker's newly hacked server. The best plan is to isolate these public-facing servers whenever possible to minimize the damage an attack can have on your network. For example, consider hosting the server in a public cloud or within an extranet in your private cloud. An extranet is a secured region of your private network where firewalls are configured to carefully inspect traffic entering and leaving the network, and on occasion, intrusion prevention systems (IPS) are implemented within the extranet to mitigate any server-to-server attacks within the extranet.

Though you probably have a firewall facing the internet within your private cloud, you may or may not have a dedicated firewall at the public cloud provider. In either case, remember that security is a process of adding layers or barriers, and you should enable and carefully configure the operating system firewall on all your servers, even those not directly exposed to the internet. The reason for the extra security is to protect against an attacker that breached one layer of your defenses.

At some point, you will need to manage your cloud-based servers. In a private cloud, you can likely perform the administration using a private network. However, unless you are operating in a hybrid cloud environment, your public servers should be behind a firewall and not directly reachable. Instead of opening remote administrative ports or services to the internet, consider establishing a virtual private network (VPN) or a dedicated wide area network (WAN) connection to your public cloud provider to allow you to manage the servers as you would in a private cloud environment.
### **Cloud Platform Security**
Cloud providers, both private and public, require a form of authentication to prove your identity and include authorization rules, such as IAM, to control what you may access. Additionally, some cloud providers may offer the ability to enable multifactor authentication (MFA) to further verify your identity. MFA is an extra step beyond a username and password. The term multifactor authentication stems from the idea that you will authenticate using different methods, namely something you know (a username and password) and either something you are (fingerprints or other biometric data) or something you have (a token or device). While private cloud providers may be able to handle biometrics as a part of your MFA configuration, most public cloud providers rely on a token or device that generates a special numeric personal identification number (PIN), which you must enter upon logging on. This token may be a keychain-like device called a key fob, or you may be able to use your mobile phone by using a virtual authenticator application that generates the PIN. In either case, this additional form of identification helps ensure that if your username and password are compromised, the attacker would still need to have access to your token or your biometrics to gain access to your account.

Humans can log in using usernames and passwords, but when an application needs to access resources within a cloud provider, it typically uses something known as a service account. These accounts are similar to regular user accounts, but they do not have passwords, and you cannot use the accounts to log in to the cloud provider’s administrative portal. The service accounts are authenticated using special strings of characters known as application programming interface (API) keys. If you have an application that requires special access to a service within the cloud provider, you could configure the application to authenticate using the service account and API key instead of hard coding a human’s username and password combination into the application.

Security would not be complete without the ability to audit the actions that users and services take throughout the course of their normal operation. Public cloud providers keep detailed audit logs of the actions taken within their system to help you account for changes and to discover any unauthorized use of privileged credentials. Typically, after you enable and configure the logging, the public cloud providers retain these audit logs for one year, though you may be able to export them and save them for a longer duration. Private cloud providers may or may not provide detailed audit logging. The level of audit logging in a private cloud is usually based on corporate governance policies and regulatory compliance requirements.

*Diagram. Public, private, and hybrid cloud deployments.*
### **Cloud Security Best Practices**
Cloud security requires attention to detail in many areas like authentication, authorization, accountability, and protecting data using encryption and regular backups. The cloud deployment model (private, public, or hybrid cloud) can have an impact on the security measures you implement as well because the deployment model indicates who is ultimately responsible for the infrastructure of the cloud services.
### **Private Cloud**
Private clouds are scalable, single-tenant clusters of computing, storage, and networking resources owned and maintained by a single company, typically (but not always) located within a data center belonging to that company. The owner of the equipment typically holds the final responsibility for all the hardware and most, if not all, of the physical data center security concerns.
### **Public Cloud**
Public clouds are hosted by companies, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP), and tend to offer highly scalable, multi-tenant solutions in data centers placed around the world. Public cloud providers generally are responsible for the physical data center security and most of the physical hardware security concerns.
### **Hybrid Cloud**
A hybrid cloud is the combination of services running in both public and private clouds. The security concerns typically fall along the lines of the owners of the equipment, much like in private and public clouds, with the addition of the data link between the public and private cloud networks, which may be maintained and secured by a third party.

Imagine you are traveling and you are at an airport. You open your laptop or phone to send an email, and instead of seeing your inbox, you get the message “unable to connect” or “no access found.” You are offline. You quickly begin looking for a way to connect to the internet. You open your computer’s list of wireless networks and see a long list of names, some with little padlock icons next to them, others without an icon. Thinking a padlock icon seems like a good indication of network security, you click on the name of one of the “locked” Wi-Fi networks only to be prompted to log in with a password that you do not know. You give up and click one of the network names without a padlock icon. Within seconds your device joins the internet, your inbox fills up, and you forget just how you managed to connect to the internet for free.

You have found one of the innumerable open, unencrypted Wi-Fi hotspots around the world, and you have just trusted your data and your identity to everyone in your immediate vicinity and anyone else that may have remote control over this Wi-Fi hotspot.

Whenever you connect to an open Wi-Fi network, you are sending and receiving all of your data in the clear, a term used to describe unencrypted traffic. Unless your application performs encryption on its own, your data is plainly visible to anyone with a computer and a wireless network card. Whether you are using a company-issued laptop containing highly confidential data, or you are just using your personal phone to browse the web, you must take wireless security very seriously. To avoid risking your data and identity, you should only connect to Wi-Fi hotspots that require you to enter a password or key to connect to them. This ensures the data is encrypted, though not all forms of encryption are equal.

The list of security protocols here (3DES and AES) are in order from weakest to strongest—you should always use AES if at all possible. The implementation of wireless encryption standards is also listed in order from weakest to strongest (WEP to WPA3).

||**WEP**|**WPA**|**WPA2**|**WPA3**|
| :- | :-: | :-: | :-: | :-: |
|**Brief description**|Ensure wired-like privacy in wireless|Based on 802.11i without requirement for new hardware|All mandatory 802.11i features and a new hardware|Announced by Wi-Fi Alliance|
|**Encryption**|RC4|TKIP + RC4|CCMP/AES|GCMP-256|
|**Authentication**|<p>WEP-Open</p><p>WEP-Shared</p>|<p>WPA-PSK</p><p>WPA-Enterprise</p>|<p>WPA2-Personal</p><p>WPA2-Enterprise</p>|<p>WPA3-Personal</p><p>WPA3-Enterprise</p>|
|**Data integrity**|CRC-32|MIC algorithm|Cipher Block Chaining Message Authentication Code (based on AES)|256-bit Broadcast/Multicast Integrity Protocol Galois Message Authentication Code (BIP-GMAC-256)|
|**Key management**|none|4-way handshake|4-way handshake|Elliptic Curve Diffie-Hellman (ECDH) exchange and Elliptic Curve Digital Signature Algorithm (ECDSA)|
### **3DES**
Triple DES (often abbreviated 3DES) is a symmetric encryption algorithm that uses the now antiquated DES (data encryption standard) algorithm three times in a row to encrypt your data. The DES algorithm uses only 56-bit encryption and can be compromised by brute force software running on modern hardware in less than a day. By utilizing multiple independent keys, 3DES increases the overall complexity of the encryption, but the encryption can still be compromised. In fact, the chance that an attacker can figure out the keys increases as you send more data in a single 3DES session. NIST, Microsoft, and many others have phased out the use of DES and 3DES, but there are still organizations and standards that use them. Also, some Wi-Fi networks still support it for backward compatibility with older devices.
### **AES**
The Advanced Encryption Standard (AES) is considered a very secure form of encryption today, although, with advances in computing power and quantum computing, it may not be considered secure forever. AES can be used with a 128-bit, 192-bit, or 256-bit key. Longer key lengths are exponentially harder to crack, but they also increase the amount of computing power required to encrypt the data. Today, most devices and websites use the 256-bit version (AES-256).

The AES standard, like 3DES and DES, is a symmetric key algorithm. One advantage of most modern processors is that they support hardware acceleration via the CPU instruction set called AES-NI (AES New Instructions). These instructions allow the CPU to process AES encryption at very fast speeds, potentially upward of 10 GB per second. This, in turn, allows a computer to encrypt its wireless network traffic using AES and then transmit it at native wireless speeds with minimal, if any, effect on performance.
### **WEP**
Wired equivalent privacy (WEP) was part of the first wireless standards proposed by the IEEE in 1997. As the name suggests, the idea was to provide the same level of security as a wired network for wireless networks. A WEP key is either 10 or 26 hexadecimal digits. Each hexadecimal digit is 4 bits. Therefore, the bit length of the encryption key is either 40-bit or 104-bit, both of which can be compromised in under a day using brute force methods on standard workstation hardware. Because all packets are encrypted by that key, WEP is very vulnerable to attack today. In 2003, the Wi-Fi Alliance, a group that certifies interoperability of devices, stated that WEP had been superseded by Wi-Fi protected access (WPA). The Wi-Fi Alliance later deprecated WEP in 2004.
### **WPA**
Wi-Fi protected access (WPA), and the subsequent standards WPA2 and WPA3 described below, were defined by the Wi-Fi Alliance and the IEEE to overcome the weaknesses of WEP. WPA was designed as a short-term bridge between the original WEP standard and the more secure IEEE 802.11i standard, which was finalized in 2004 (now known as WPA2). WPA is based on the draft IEEE 802.11i standard that was released in 2003.

One of the most noticeable differences between WEP and WPA is the key used to access the wireless network. Where WEP relies on a fixed-length hexadecimal preshared key, WPA uses a variable-length alphanumeric passphrase, which can range from 8 to 63 characters in length.

Another subtler, but even more powerful, upgrade for WPA is the addition of an encryption process known as temporal key integrity protocol (TKIP). Unlike WEP where all packets on the network are encrypted using the same encryption key, TKIP gives WPA a significant security boost by generating a new 128-bit encryption key for every packet sent on the network. This means that instead of cracking one key to decrypt the traffic, an attacker must crack potentially millions of keys. That said, over time, several flaws and weaknesses have been found in the WPA protocol, which prompted the release of WPA2 and then WPA3.
### **WPA2**
Introduced in 2004 by the Wi-Fi Alliance, WPA2 quickly became the standard for wireless security for the next 15 years. The major difference between WPA2 and the original WPA is the mandatory support for Counter Mode Cipher Block Chaining Message Authentication Code Protocol (CCMP for short), which is part of the AES encryption standard. It is designed to provide data confidentiality, authentication, and access control to the network. Eventually, WPA2 developed weaknesses that led to the development of WPA3 as its replacement.
### **WPA3**
WPA3 was released in January 2018 to address the weaknesses of WPA2. Specifically, it increases the minimum key strength to 192-bits for enterprise mode connections, which are often used in organizations instead of the alternate personal mode available in WPA standards.

A much more noticeable change in WPA3 is the elimination of the passphrase or key that WEP, WPA, and WPA2 use to allow computers to join a personal-mode wireless network. In WPA3, all devices now use the simultaneous authentication of equals (SAE) method to exchange the network key as defined in the IEEE 802.11-2016 standard. The SAE method ensures the initial key exchange in personal mode is more secure by eliminating the need to tell others the key before they connect to the network.

WPA3 also implements another method known as forward secrecy or perfect forward secrecy (PFS), which ensures that even if one session key is compromised, that compromised key will only affect data exchanged in that encryption session, not in any past or future sessions.

One other improvement in this standard, as defined in the 802.11w specification, is the encryption of management frames, such as de-associating from the network (see Deauth Attack section below for an exploit of this behavior in networks without WPA3). This is only possible once encryption has been established, so devices are still vulnerable to management frame attacks prior to the session encryption negotiation.

All new devices and routers will support the WPA3 standard and, as with the other standards listed here, you should always choose the latest standard supported by the wireless devices and routers.

*Diagram. A wireless network in ad-hoc mode.*
### **Ad-hoc**
In ad-hoc mode, all wireless communication is performed in a peer-to-peer fashion and does not require or involve a WAP. Ad-hoc wireless networks are rarely used in homes or offices, but they can be helpful in setting up a new device, such as a printer by connecting them directly, much as one would do with a physical cable. Ad-hoc networks are also occasionally used to transfer files between devices, such as between a camera and a laptop.
### **Infrastructure**
*Diagram. Wireless infrastructure mode.*

In almost all cases, a WAP or wireless router is used to connect wireless devices to the network. The WAP acts like an Ethernet switch in wired networking and often has a physical cable that connects it to the rest of the network. A wireless router is a WAP and a router combined into a single device and is most often used in home and small business environments to connect to the internet while also providing wireless connectivity for nearby devices.

Imagine you are a network administrator for a company with a sprawling office complex. You have network cables run to each office and cubicle in the building, but not all the offices and cubicles are occupied by employees. Some of these areas are often left unoccupied, which has encouraged temporary workers, such as contractors and vendors, to use these otherwise unused network ports to access the internet. The problem is that they also have access to the corporate network, creating a security concern for management. You need to find a way to stop the unauthorized users while being unobtrusive to the legitimate users in the company.

You could just disconnect each of the network cables while they are not in use, but that is a labor-intensive task that may not get performed quickly enough to satisfy your requirement. You need a more dynamic approach to the solution, one that could extend beyond the copper wiring of the building to include wireless connections as well. You need some form of network authentication and authorization.

The 802.1x security standard was designed to fit that exact situation. It provides network access control at the port level, whether physical or wireless, and it provides an authentication standard based on the Extensible Authentication Protocol (EAP). Authentication is typically done via a username and password, but you can also configure it to use public key infrastructure (PKI) certificates.

*Diagram. 802.1X Authentication.*

802\.1x grants trusted network access as follows:

1. The client requests access to the network via a WAP or a wired Ethernet switch and then provides credentials for the network access.
1. The WAP or switch forwards the network access request to a special authentication server running remote authentication dial-in user service (RADIUS) or EAP, which then validates the credentials and determines the user’s authorization based on policies defined by a network administrator.
1. If the credentials are validated and the user is authorized, the WAP or switch grants network access to the user.
1. If the user is not authenticated properly or does not have the authorization to access that network, the WAP or switch will block network access to the user.

The authorization policies used in 802.1x systems can also check the version of antivirus or malware scanners on a computer. If the user’s device does not conform to the corporate standard, the network administrator may configure the policy to permit limited network access, such as access to a server to update the antivirus software.
### **Wireless Attacks**
While the security standards and encryption algorithms discussed above may prevent most wireless network attacks, there are some attacks that remain challenging. Unlike cabled networks that require an attacker to gain physical access to wiring before the attacker can cause trouble, you may not even see the attacker in a wireless network. The attacker only needs to be within range of a WAP, which may have a range of hundreds of feet. Your attacker may be sitting comfortably in their car while hacking your network.

*Diagram: Denial of Service (DoS) Attack*
### **Deauth Attack**
Deauthentication (abbreviated deauth) is a denial-of-service (DoS) attack where the attacker can force any client (or even every client) off of the network. Worse yet, the attacker does not even need to be on the network they are attacking. That is right, the attacker can kick anyone off the network without even joining that wireless network. But what is the point? Users can simply reconnect. While that is true, there are several reasons an attacker might want to perform a deauth attack:

- To prevent access to the network (the main point of any DoS attack)
- To force users to reconnect and have them connect to the attacker’s access point instead (see Fake Access section for more details)
- To capture the 4-way handshake of WPA to gain intelligence that allows the attacker to hack into the WAP to gain access to the corporate network

The simplest defense is to use WPA3 security on your WAPs because in WPA3, the management packets are encrypted. If it is not possible to use WPA3, at least use WPA2 to make sure the data traffic is encrypted. Just be aware that you are being targeted, so you should consider upgrading your wireless network so that it can operate using WPA3 security. Additionally, use a long, complex passphrase to gain access to the network. The passphrase should contain upper and lower case letters, numbers, and special characters.
### **Fake Access**
In this attack type, an attacker sets up an illegitimate wireless network using their own WAP and may even share their own cellular data to create an internet hotspot. The attacker usually opens this network without any security or authentication so as to entice people in a hurry to connect to the attacker’s rogue WAP.

When the unwitting victim connects to the rogue WAP, the attacker becomes privy to all the data the victim’s computer sends or receives. This also means the attacker could redirect the data to a different location, or even modify the contents of the data. Any unencrypted traffic is plainly visible to the attacker—usernames and passwords, credit cards, or any other data not already encrypted. This attack is very easy to set up, requires little technical expertise, and can yield a lot of information for the attacker.

If you must use unsecured networks, create a virtual private network (VPN) tunnel on your device that connects to a VPN service at your office. The VPN will encrypt all traffic sent and received over the wireless network. But the best defense against this kind of attack is to train your employees to never use unsecured Wi-Fi hotspots, like the one you clicked on to send that email at the airport.
### **User Authentication and Access Control**
Authentication, authorization, and accounting (AAA) describes the process of granting or denying access to data and network resources as well as verifying that the security controls are working properly. The first step is authentication, in which you confirm the user is who they claim to be. Next is authorization, where you define what that user is able to access. Finally, you must account for and report on the access that a user has been granted, including how often the user accesses the resource or data.

User authentication and access control refer to the way we access and get access to computer resources. It’s part of the category that includes authentication, authorization, and accounting or AAA.

Authentication, the first area, is about proving our identity. Typically, this involves using a name and a password to log into a phone, mobile device, laptop, website, or Google account. A username and a password, also referred to as login credentials, prove you are who you say you are.

Once a system validates user identity, the next step is authorization, which refers to what the user is allowed to do. Permission to do various tasks includes whether or not the user can open, delete, change, or print a document. Another type of authorization might be the ability to do something with a virtual machine— or to create a new one. Regardless of the situation, authorization is about our ability to do whatever we want to do with a particular system.

The third part of AAA is accounting, which means verifying that the rules we created are being enforced as expected, without unauthorized access. It also permits auditing of authorized access to figure out if there really was a breach, and, if so, when it happened or how it happened. Accounting helps us learn from these successful attacks, harden our defenses, and prevent similar future attacks. Another advantage of accounting is verifying that the things we think are happening, are happening. They say an administrator goes rogue and directs us to start copying data out of a system. We would receive that notification, note that it’s abnormal behavior, and have a second person validate, that yes, it's OK or no, something weird may be occurring and requires further investigation.

Now that we’ve discussed authentication, authorization, and accounting, let’s look more closely at the idea of access control and proving authentication. In reality, it's not difficult to figure out another person’s username and password. For example, a hacker may attempt to log into various websites using a known email address. Once the hacker confirms that the email address corresponds to an account for a particular website, the hacker would then attempt to determine the user's password for that account. Many people use the same password repeatedly, essentially saying, “Please hack me,” because eventually, someone will try the password that worked in one place to access another. A more secure option with better control is to use a password manager instead. However, passwords can be compromised or guessed.

Multi-Factor Authentication or MFA takes access control a step further. Sometimes called Two-Factor Authentication or 2FA, it combines something we know with something we have, such as an authentication device. What we know is the password; what we have can be a physical key fob with a code that changes every thirty seconds or a virtual version that runs on a smartphone. Other ways of providing multi-factor authentication include fingerprint, retina, or face scans. Each method proves you are who you say you are. Even if somebody steals your password, unless they also take your phone, fob, or whatever, they won’t have the ability to log in and act like you. The bigger a person’s potential attack surface, the more crucial is multi-factor authentication. Adding multi-factor authentication for sensitive sites like banking and credit card accounts enhances security, and everyone should use it.

To summarize, user authentication and access control refer to the way we access and get access to computer resources. Authentication is about proving our identity, which we typically do with a username and password or, more securely, with multi-factor authentication. Authorization is about our ability to access a particular system and use it correctly as per the set organizational rules. Finally, accounting verifies that users are following the rules we created as expected and that unauthorized users don’t have access to the system.
### **Authentication**
When you think of authentication, you may think of a person gaining access to resources, but authentication is actually just the process of confirming a person’s identity. A system can confirm your identity via usernames and passwords or with certificates, as is the case with public key infrastructure (PKI).

Microsoft Active Directory is an example of an authentication system that confirms the identity of users via passwords. Another example is your web browser, which can use PKI certificates to validate the identity of websites, such as those belonging to your bank or healthcare provider.
### **Authorization**
Once the identity of the user has been confirmed through authentication, the authorization system steps in to determine what the user may access. For example, can the user access resources in a particular subnet? Does the user have access to a particular server or file? If data access is in question, can the user write to or delete the data, or is the access read-only? The list of questions (and potential restrictions) goes on almost endlessly.

It is critical to apply restrictive permissions to your data and to carefully secure access to your servers and network devices. If left unrestricted, users could intentionally or accidentally gain access to confidential data and perhaps even post the information publicly. Keep in mind that permitting extra or unneeded access to data or servers is not just a risk for data leaks or breaches of confidentiality. You must also consider the potential for accidents. For instance, if a user’s computer becomes infected with malware, that user may accidentally infect all the data files that they can access on the network.

*Diagram. A typical AAA interaction.*Diagram DescriptionFirst, the client sends an access request. The server responds with access-accept (with exec authorization in attributes). The client then sends an accounting request (start). The server sends the accounting response to client. The client sends another accounting request (stop). The server sends the accounting response to client.
### **Accounting**
In a perfect world, accounting (auditing) would not be needed, but then again, in a perfect world you would not need authentication or authorization because everyone would be properly trained and live by the rules; unfortunately, this is not a perfect world. Thus, you have a need for the critical nature of accounting to verify that the restrictions you thought were in place are working as expected and that there is not attempted or actual unauthorized access. Accounting also includes verifying the correct access control settings on data files, providing a forensic trail after a security breach to determine how the attacker got in (to harden defenses for the future) and what they accessed (for damage control and potential changes to permissions in the future). The data gathered during the accounting process should be stored in a different location than the data that is being audited. In this way, an attacker cannot easily access your security logs or records and then change them to hide their attack. Additionally, these logs should be stored in an immutable, or unchangeable, form to prevent anything from happening, including the auditing system from changing or deleting the audit logs. Read access to this data should also be limited to those with a need to know; this typically includes security auditors and administrators.
### **MFA**
Multi-factor authentication (MFA), sometimes called 2-factor authentication (2FA), is an optional, but highly recommended, add-on to the authentication process described previously. If the authentication system requires MFA for your user account, you will see an additional prompt after you enter your username and password. The prompt may ask for something you have (such as a PIN on a key fob token) or something you are (a facial or retinal scan or a fingerprint).

If you have a recent mobile device from Apple, you probably already use Touch ID or Face ID. Many of the more expensive Android devices have similar features. In those systems, your username and password are stored within a secure area of the device that can only be unlocked using your fingerprint or a facial recognition scan. Once unlocked, your mobile device sends the username and password to the site you are attempting to access. This simplifies the way you access websites and applications because it minimizes the number of passwords that you must remember. In a similar way, you can unlock a computer running MacOS by using an Apple Watch. This is referred to as proximity-based security and works automatically if your watch is within a certain range of the MacOS computer. Apple is not the only one to simplify the process of logging into your computer. Microsoft Windows computers have a similar mechanism called Windows Hello, which can use your fingerprint or facial recognition to log in to your device.

The point of MFA is that even if someone steals your password through social engineering, brute force attacks, or any other way, they cannot access your device or data without the key fob or without your fingerprint or face. This makes it much more difficult for a potential attacker to impersonate you and maliciously access your account.
### **Why Harden Devices?**
Network devices, such as switches, routers, and firewalls have access to all data sent through them, which means they have access to a lot of your data. If these devices were compromised by a malicious user or attacker, the attacker would gain access to all of that data plus other information that could allow the attacker to change firewall rules and cause other damage to your network and the devices. These threats are even more potent when wireless networks are involved because just being in the vicinity may be enough to allow an attacker to gain access, change device settings, and eavesdrop on your network traffic.

This is why you must carefully and regularly review the security settings on your network devices, update the device software, and test the security of the device by attempting to breach its defenses. This process is known as hardening the security of a device, though the process also applies to servers and workstations.
### **How to Harden Devices**
There are some general security hardening steps that should be performed on all network devices as well as a few device-specific steps that differ by the type of device.
### **Change Default Passwords**
First and foremost, when you install a new or even previously used network device, be sure to change the device’s password. Never leave the device configured with its factory-default password. Attackers can easily search the internet and find the default password of nearly every brand and model of network device ever made. Unfortunately, due to laziness or ignorance on the part of the network administrator, attackers often succeed in logging in using the default password.
### **Remove Unnecessary Logins**
Closely related to the last point is to remove any unnecessary accounts (including the default users if possible) from the device. Unnecessary accounts, in this context, are those not used by you (at home) or the network administration team responsible for the device in question. Of course, you will first need to create replacement users and passwords for them, but this makes it that much more difficult for the attacker when they try and log in to your device as an admin, but you have deleted that account and created one with another name.

A second part of this recommendation is to periodically review the list of authorized users in each device and remove those that no longer need access (because they changed job roles, left the company, etc.). This is also a good time to make sure that no unrecognized accounts are on the device as well and to remove any that are found immediately along with changing all passwords and further hardening the device as quickly as possible to limit potential damage.
### **Enforce a Strong Password Policy**
Devices should require strong passwords for all users. If possible, enforce this policy through the device’s operating system; if not, make a company policy for everyone (including the administrators) to have a complex password (a long password of at least 10–12 characters with a combination of uppercase and lowercase letters, numbers, and special symbols, subject to what the device supports). Additionally, each user should be required to change their password frequently (typically every 30-90 days, depending on the organization, sensitivity of the data being stored and moved across the network, etc.) to further limit damage if something is compromised. If the device can provide a report on the last time a password was changed, run that report and make sure that the policy is being followed.

If the device supports multi-factor authentication (MFA), be sure to require that as well and configure it properly for every user. Text messaging and email are not considered strong forms of MFA but are better than nothing if that is all that is available. Virtual MFA applications or physical tokens are better, if possible.
### **Remove Unnecessary Services**
If there are optional features installed on the device that you are not using, disable or remove them (depending on the device and platform) to reduce the risk of attack that service opens up. This is true for all of your devices, but especially the network ones. That said, if there are features that harden the device, you may wish to enable or add them to strengthen the defenses of the device.
### **Keep Patches Up to Date**
Be sure to patch every device to get the latest security updates installed before an attacker can exploit a security flaw. This is true not only for servers and workstations, but for every network device, storage device, wireless router, etc. that can be updated. Many times, security researchers discover a vulnerability and notify the manufacturer so they can fix it before publicly disclosing it 90 days or so after discovery. The manufacturer needs time to come up with and test the patch, make it available, and then you need to get it installed before an attacker finds out about it and starts looking for victims.
### **Limit Physical Access to the Device**
One of the basic tenants of security is that physical access equals access. As has been explained elsewhere in this course, if someone has physical access to a device, they can break into it if given enough time. Many network devices also have special ports or reset buttons on them that could allow an attacker who has physical access to make changes quite easily. Lock up your network devices and be sure to observe anyone who physically interacts with the device.
### **Only Allow Changes from a Trusted Network**
For all devices, but especially firewalls and wireless devices that are often exposed to the internet and the wider public at large, there are a couple of other precautions that should be followed:

- Disallow any changes from the public side of the network device.
- Disallow changes to a wireless access point or wireless router from other wireless devices.

The purpose of these restrictions is to ensure that the user is authorized and inside the network (see the previous point about limiting physical access in this regard as well).
### **Require Encryption for Wireless Networks**
Be sure to configure and enable WPA2 (or better yet, WPA3) so that the traffic crossing the network is encrypted as discussed elsewhere in this course. You may have seen networks on your phone, tablet, or computer that do not have the lock indicator beside the network name—they are open public networks. Even if you are redirected to a log-in page, that is usually for legal reasons, not for any encryption purposes. Never send confidential information across an unencrypted network, especially a public one.
### **Audit Access**
While all of the above recommendations are proactive in nature, it is possible that something slips through. To help you look for suspicious activity and to do a forensic analysis of when a device was breached, from where, and the methods used, you will need audit logs. Be sure to collect these logs and store them off of the device (and thus, out of the attacker’s reach). In fact, the lack of audit logs being created when they have been in the past can be a red flag that something is wrong. Syslog is a common way to gather the logs and send them to a Syslog server for storage. There are many applications, both commercial and open-source, that can review these logs and alert you when anomalies are detected and should be further investigated by a person.
###
###
### **Backup**
Keep backup copies of any device configuration files in case they need to be restored due to misconfiguration, attack, or device replacement, among other reasons. Devices on the network should also be backed up and stored remotely. These backups can be stored in the datacenter, but copies should be stored off-site in case of fire, flood, or other natural disasters as well as to protect against other problems that may affect the entire data center. This is your last line of defense in many cases.



